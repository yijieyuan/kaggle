{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ed4d7e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-13T03:36:13.418540Z",
     "iopub.status.busy": "2025-01-13T03:36:13.418248Z",
     "iopub.status.idle": "2025-01-13T03:36:17.595937Z",
     "shell.execute_reply": "2025-01-13T03:36:17.594970Z"
    },
    "papermill": {
     "duration": 4.184796,
     "end_time": "2025-01-13T03:36:17.597276",
     "exception": false,
     "start_time": "2025-01-13T03:36:13.412480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoteBook Start Time 2025-01-12 22:36\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import time\n",
    "import pytz\n",
    "import torch\n",
    "import random\n",
    "import shutil\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch.nn as nn\n",
    "import kaggle_evaluation.jane_street_inference_server\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "# Define EST timezone\n",
    "est = pytz.timezone('US/Eastern')\n",
    "notebook_start_time = datetime.now(est)\n",
    "time_to_minutes = notebook_start_time.strftime(\"%Y-%m-%d %H:%M\")\n",
    "print(\"NoteBook Start Time\", time_to_minutes)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3642d4fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T03:36:17.604172Z",
     "iopub.status.busy": "2025-01-13T03:36:17.603792Z",
     "iopub.status.idle": "2025-01-13T03:36:17.608529Z",
     "shell.execute_reply": "2025-01-13T03:36:17.607855Z"
    },
    "papermill": {
     "duration": 0.00945,
     "end_time": "2025-01-13T03:36:17.609779",
     "exception": false,
     "start_time": "2025-01-13T03:36:17.600329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the R² metric as a loss function (inverted for minimization)\n",
    "def weighted_r2_loss(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Calculate the weighted R² metric as a loss for minimization.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Ground truth values, shape (batch_size,).\n",
    "        y_pred (torch.Tensor): Predicted values, shape (batch_size,).\n",
    "        weights (torch.Tensor): Sample weights, shape (batch_size,).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Weighted R² value (negative for minimization).\n",
    "    \"\"\"\n",
    "    numerator = torch.sum(weights * (y_true - y_pred) ** 2)\n",
    "    denominator = torch.sum(weights * y_true ** 2) + 1e-38\n",
    "    r2 = 1 - numerator / denominator\n",
    "    return -r2  # Negative for minimization\n",
    "\n",
    "# Define normalization functions\n",
    "def z_score(col, mean_val, std_val):\n",
    "    \"\"\"Return z-scored column: (x - mean) / std.\"\"\"\n",
    "    return (col - mean_val) / std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1639d804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T03:36:17.615685Z",
     "iopub.status.busy": "2025-01-13T03:36:17.615483Z",
     "iopub.status.idle": "2025-01-13T03:36:17.622508Z",
     "shell.execute_reply": "2025-01-13T03:36:17.621902Z"
    },
    "papermill": {
     "duration": 0.011353,
     "end_time": "2025-01-13T03:36:17.623783",
     "exception": false,
     "start_time": "2025-01-13T03:36:17.612430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim = 81,\n",
    "                 hidden_dims = [256,256,256],\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.epoch = 0\n",
    "\n",
    "        layers = []\n",
    "        in_dim = self.in_dim\n",
    "    \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim, bias=True))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim, 1))  # Output layer\n",
    "        layers.append(nn.Tanh())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: (batch_size, input_dim)\n",
    "        '''\n",
    "        return 5 * self.model(x[:, :self.in_dim]).squeeze(-1)\n",
    "\n",
    "class RMFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Holds X_list, y_list, w_list, each item is a tensor already.\n",
    "    Avoids deep copying to save memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_list, y_list, w_list):\n",
    "        self.X_list = X_list\n",
    "        self.y_list = y_list\n",
    "        self.w_list = w_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_list[idx], self.y_list[idx], self.w_list[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1345e39a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T03:36:17.630050Z",
     "iopub.status.busy": "2025-01-13T03:36:17.629802Z",
     "iopub.status.idle": "2025-01-13T03:36:17.722723Z",
     "shell.execute_reply": "2025-01-13T03:36:17.721978Z"
    },
    "papermill": {
     "duration": 0.097879,
     "end_time": "2025-01-13T03:36:17.724396",
     "exception": false,
     "start_time": "2025-01-13T03:36:17.626517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dirpath_initial_parquet1 = '/kaggle/input/js-rmf-0-999'\n",
    "dirpath_initial_parquet2 = '/kaggle/input/js-rmf-1000-1698'\n",
    "dirpath_stats = '/kaggle/input/js-rmf-data-stats'\n",
    "global_stats = (pl.read_csv(os.path.join(dirpath_stats, \"global_stats_with_lags.csv\")))\n",
    "\n",
    "## Configuration\n",
    "num_train_days = 500\n",
    "num_gap_days = 200\n",
    "num_total_days = 1699\n",
    "num_splits = 10\n",
    "\n",
    "# Training\n",
    "batch_size = 1\n",
    "lr = 1e-4\n",
    "lr_finetune = 1e-4\n",
    "lambda_reg = 1e-4\n",
    "num_epochs = 9\n",
    "\n",
    "# Define feature groups\n",
    "ORIGINAL_FEATURES = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "EXTRA_FEATURES = ['weight', 'time_id']\n",
    "ALL_FEATURES = ORIGINAL_FEATURES + EXTRA_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1f7946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T03:36:17.731166Z",
     "iopub.status.busy": "2025-01-13T03:36:17.730890Z",
     "iopub.status.idle": "2025-01-13T03:37:14.430714Z",
     "shell.execute_reply": "2025-01-13T03:37:14.429076Z"
    },
    "papermill": {
     "duration": 56.704851,
     "end_time": "2025-01-13T03:37:14.432276",
     "exception": false,
     "start_time": "2025-01-13T03:36:17.727425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight mean: 2.00944066, std: 1.129388213, min: 0.149966657, max: 10.24041939\n",
      "TimeID mean: 468.7057222, std: 272.5186966, min: 0.0, max: 967.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:56<00:00,  8.82it/s]\n"
     ]
    }
   ],
   "source": [
    "global_stats_dict = {}\n",
    "for row in global_stats.to_dicts():\n",
    "    # row[\"feature\"] might look like \"feature_00\", \"weight\", etc.\n",
    "    global_stats_dict[row[\"feature\"]] = {\n",
    "        \"mean\": row[\"mean\"],\n",
    "        \"std\": row[\"std\"],\n",
    "        \"min\": row[\"min\"],\n",
    "        \"max\": row[\"max\"]\n",
    "    }\n",
    "\n",
    "weight_mean = global_stats_dict['weight']['mean']\n",
    "weight_std = global_stats_dict['weight']['std']\n",
    "weight_min = global_stats_dict['weight']['min']\n",
    "weight_max = global_stats_dict['weight']['max']\n",
    "time_mean = global_stats_dict['time_id']['mean']\n",
    "time_std = global_stats_dict['time_id']['std']\n",
    "time_min = global_stats_dict['time_id']['min']\n",
    "time_max = global_stats_dict['time_id']['max']\n",
    "print(f\"Weight mean: {weight_mean}, std: {weight_std}, min: {weight_min}, max: {weight_max}\")\n",
    "print(f\"TimeID mean: {time_mean}, std: {time_std}, min: {time_min}, max: {time_max}\")\n",
    "\n",
    "def process_and_update(file_paths, num_splits=num_splits, X_list=None, y_list=None, w_list=None):\n",
    "    # Avoid using mutable default arguments\n",
    "    if X_list is None:\n",
    "        X_list = []\n",
    "    if y_list is None:\n",
    "        y_list = []\n",
    "    if w_list is None:\n",
    "        w_list = []\n",
    "\n",
    "    for file_path in tqdm(file_paths):\n",
    "        day_df = pl.read_parquet(file_path)\n",
    "        day_df = day_df.select([day_df[col].cast(pl.Float32).alias(col) for col in day_df.columns])\n",
    "\n",
    "        for feature in ALL_FEATURES:\n",
    "            stats = global_stats_dict[feature]\n",
    "            mean_val = stats[\"mean\"]\n",
    "            std_val  = stats[\"std\"]\n",
    "            day_df = day_df.with_columns(\n",
    "                z_score(day_df[feature], mean_val, std_val).alias(feature)\n",
    "            )\n",
    "\n",
    "        day_df = day_df.fill_null(0)\n",
    "        day_df = day_df.sort(\"time_id\")\n",
    "        unique_time_ids = day_df['time_id'].unique()\n",
    "        num_time_ids = len(unique_time_ids)\n",
    "        split_size = num_time_ids // num_splits\n",
    "\n",
    "        for i in range(num_splits):\n",
    "            start = i * split_size\n",
    "            end = (i + 1) * split_size if i < num_splits - 1 else num_time_ids\n",
    "            if start >= num_time_ids:\n",
    "                break\n",
    "            subset_time_ids = unique_time_ids[start:end]\n",
    "            subset_df = day_df.filter(pl.col(\"time_id\").is_in(subset_time_ids))\n",
    "            \n",
    "            X = torch.tensor(subset_df.select(ALL_FEATURES).to_numpy(), dtype=torch.float32)\n",
    "            y = torch.tensor(subset_df.select('responder_6').to_numpy(), dtype=torch.float32).squeeze()\n",
    "            w = torch.tensor(subset_df.select('weight').to_numpy(), dtype=torch.float32).squeeze() * weight_std + weight_mean\n",
    "\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            w_list.append(w)\n",
    "\n",
    "    return X_list, y_list, w_list\n",
    "\n",
    "initial_file_paths = [os.path.join(dirpath_initial_parquet1, f\"{i}.parquet\") for i in range(1000)] + [os.path.join(dirpath_initial_parquet2, f\"{i}.parquet\") for i in range(1000, 1699)]\n",
    "initial_file_paths = initial_file_paths[num_total_days-num_train_days:num_total_days]\n",
    "X_list, y_list, w_list = process_and_update(initial_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b876b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T03:37:14.486387Z",
     "iopub.status.busy": "2025-01-13T03:37:14.486084Z",
     "iopub.status.idle": "2025-01-13T03:39:54.998499Z",
     "shell.execute_reply": "2025-01-13T03:39:54.997511Z"
    },
    "papermill": {
     "duration": 160.539015,
     "end_time": "2025-01-13T03:39:54.999729",
     "exception": false,
     "start_time": "2025-01-13T03:37:14.460714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:18<02:25, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:37:33 | Epoch 1: train_R² = -0.019330, cpu memory usage=6.649105 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:36<02:06, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:37:51 | Epoch 2: train_R² = -0.006947, cpu memory usage=6.649334 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:53<01:47, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:38:09 | Epoch 3: train_R² = -0.000621, cpu memory usage=6.648209 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [01:11<01:28, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:38:27 | Epoch 4: train_R² = 0.003240, cpu memory usage=6.648235 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [01:28<01:10, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:38:44 | Epoch 5: train_R² = 0.005579, cpu memory usage=6.649399 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [01:46<00:52, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:39:02 | Epoch 6: train_R² = 0.007482, cpu memory usage=6.648193 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [02:04<00:35, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:39:19 | Epoch 7: train_R² = 0.009121, cpu memory usage=6.649170 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [02:21<00:17, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:39:37 | Epoch 8: train_R² = 0.010041, cpu memory usage=6.648239 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:39<00:00, 17.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:39:54 | Epoch 9: train_R² = 0.011133, cpu memory usage=6.649120 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 9\n",
    "input_dim = len(ALL_FEATURES)\n",
    "model = MLP(input_dim=input_dim)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "X_list_train, y_list_train, w_list_train = X_list[:num_train_days * num_splits], y_list[:num_train_days * num_splits], w_list[:num_train_days * num_splits]\n",
    "assert len(X_list_train) == len(y_list_train) == len(w_list_train) == num_train_days * num_splits\n",
    "train_dataset = RMFDataset(X_list_train, y_list_train, w_list_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch, (X_batch, y_batch, w_batch) in enumerate(train_loader, 1):\n",
    "        X_batch, y_batch, w_batch = X_batch[0].to(device), y_batch[0].to(device), w_batch[0].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        l2_reg = sum(param.pow(2).sum() for param in model.parameters())\n",
    "        loss = weighted_r2_loss(y_batch, y_pred, w_batch) + lambda_reg * l2_reg\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_R2 = -total_loss / len(train_loader)\n",
    "        \n",
    "    process=psutil.Process()\n",
    "    mem=process.memory_info().rss/(1024**3); \n",
    "    print(f\"Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())} | Epoch {epoch + 1}: train_R² = {train_R2:.6f}, cpu memory usage={mem:.6f} GB\")\n",
    "            \n",
    "predictor1 = copy.deepcopy(model)\n",
    "predictor1.to(device)\n",
    "refiner1 = torch.optim.Adam(predictor1.parameters(), lr=lr_finetune) # Optimizer for find-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7015d9be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T03:39:55.050950Z",
     "iopub.status.busy": "2025-01-13T03:39:55.050471Z",
     "iopub.status.idle": "2025-01-13T03:42:52.229780Z",
     "shell.execute_reply": "2025-01-13T03:42:52.228807Z"
    },
    "papermill": {
     "duration": 177.231435,
     "end_time": "2025-01-13T03:42:52.255945",
     "exception": false,
     "start_time": "2025-01-13T03:39:55.024510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:17<02:37, 17.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:40:12 | Epoch 1: train_R² = -0.018912, cpu memory usage=6.649242 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:35<02:21, 17.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:40:30 | Epoch 2: train_R² = -0.006921, cpu memory usage=6.649242 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:53<02:04, 17.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:40:48 | Epoch 3: train_R² = -0.000830, cpu memory usage=6.649487 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:10<01:46, 17.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:41:05 | Epoch 4: train_R² = 0.003008, cpu memory usage=6.649487 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:28<01:28, 17.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:41:23 | Epoch 5: train_R² = 0.005643, cpu memory usage=6.649208 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:46<01:10, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:41:41 | Epoch 6: train_R² = 0.007585, cpu memory usage=6.649208 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:04<00:53, 17.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:41:59 | Epoch 7: train_R² = 0.008984, cpu memory usage=6.648338 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:22<00:35, 17.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:42:17 | Epoch 8: train_R² = 0.010209, cpu memory usage=6.649315 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:39<00:17, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:42:35 | Epoch 9: train_R² = 0.011263, cpu memory usage=6.649437 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:57<00:00, 17.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:42:52 | Epoch 10: train_R² = 0.012144, cpu memory usage=6.649437 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "input_dim = len(ALL_FEATURES)\n",
    "model = MLP(input_dim=input_dim)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "X_list_train, y_list_train, w_list_train = X_list[:num_train_days * num_splits], y_list[:num_train_days * num_splits], w_list[:num_train_days * num_splits]\n",
    "assert len(X_list_train) == len(y_list_train) == len(w_list_train) == num_train_days * num_splits\n",
    "train_dataset = RMFDataset(X_list_train, y_list_train, w_list_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch, (X_batch, y_batch, w_batch) in enumerate(train_loader, 1):\n",
    "        X_batch, y_batch, w_batch = X_batch[0].to(device), y_batch[0].to(device), w_batch[0].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        l2_reg = sum(param.pow(2).sum() for param in model.parameters())\n",
    "        loss = weighted_r2_loss(y_batch, y_pred, w_batch) + lambda_reg * l2_reg\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_R2 = -total_loss / len(train_loader)\n",
    "        \n",
    "    process=psutil.Process()\n",
    "    mem=process.memory_info().rss/(1024**3); \n",
    "    print(f\"Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())} | Epoch {epoch + 1}: train_R² = {train_R2:.6f}, cpu memory usage={mem:.6f} GB\")\n",
    "            \n",
    "predictor2 = copy.deepcopy(model)\n",
    "predictor2.to(device)\n",
    "refiner2 = torch.optim.Adam(predictor2.parameters(), lr=lr_finetune) # Optimizer for find-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "240d13db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T03:42:52.308671Z",
     "iopub.status.busy": "2025-01-13T03:42:52.308350Z",
     "iopub.status.idle": "2025-01-13T03:46:08.561925Z",
     "shell.execute_reply": "2025-01-13T03:46:08.560616Z"
    },
    "papermill": {
     "duration": 196.280992,
     "end_time": "2025-01-13T03:46:08.563399",
     "exception": false,
     "start_time": "2025-01-13T03:42:52.282407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:17<02:56, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:43:09 | Epoch 1: train_R² = -0.017902, cpu memory usage=6.652992 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:35<02:38, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:43:27 | Epoch 2: train_R² = -0.006543, cpu memory usage=6.650295 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:52<02:21, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:43:45 | Epoch 3: train_R² = -0.000394, cpu memory usage=6.649364 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [01:11<02:05, 17.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:44:03 | Epoch 4: train_R² = 0.003032, cpu memory usage=6.651665 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [01:28<01:47, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:44:21 | Epoch 5: train_R² = 0.005521, cpu memory usage=6.649212 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 6/11 [01:46<01:29, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:44:39 | Epoch 6: train_R² = 0.007510, cpu memory usage=6.650585 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [02:04<01:11, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:44:56 | Epoch 7: train_R² = 0.009056, cpu memory usage=6.649544 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 8/11 [02:23<00:54, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:45:15 | Epoch 8: train_R² = 0.010154, cpu memory usage=6.651478 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [02:40<00:35, 17.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:45:32 | Epoch 9: train_R² = 0.011151, cpu memory usage=6.650494 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [02:58<00:17, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:45:50 | Epoch 10: train_R² = 0.012322, cpu memory usage=6.648247 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [03:16<00:00, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2025-01-13 03:46:08 | Epoch 11: train_R² = 0.013187, cpu memory usage=6.651562 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 11\n",
    "input_dim = len(ALL_FEATURES)\n",
    "model = MLP(input_dim=input_dim)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "X_list_train, y_list_train, w_list_train = X_list[:num_train_days * num_splits], y_list[:num_train_days * num_splits], w_list[:num_train_days * num_splits]\n",
    "assert len(X_list_train) == len(y_list_train) == len(w_list_train) == num_train_days * num_splits\n",
    "train_dataset = RMFDataset(X_list_train, y_list_train, w_list_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch, (X_batch, y_batch, w_batch) in enumerate(train_loader, 1):\n",
    "        X_batch, y_batch, w_batch = X_batch[0].to(device), y_batch[0].to(device), w_batch[0].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        l2_reg = sum(param.pow(2).sum() for param in model.parameters())\n",
    "        loss = weighted_r2_loss(y_batch, y_pred, w_batch) + lambda_reg * l2_reg\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_R2 = -total_loss / len(train_loader)\n",
    "        \n",
    "    process=psutil.Process()\n",
    "    mem=process.memory_info().rss/(1024**3); \n",
    "    print(f\"Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())} | Epoch {epoch + 1}: train_R² = {train_R2:.6f}, cpu memory usage={mem:.6f} GB\")\n",
    "            \n",
    "predictor3 = copy.deepcopy(model)\n",
    "predictor3.to(device)\n",
    "refiner3 = torch.optim.Adam(predictor3.parameters(), lr=lr_finetune) # Optimizer for find-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b89848b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T03:46:08.618095Z",
     "iopub.status.busy": "2025-01-13T03:46:08.617762Z",
     "iopub.status.idle": "2025-01-13T03:46:08.659153Z",
     "shell.execute_reply": "2025-01-13T03:46:08.658461Z"
    },
    "papermill": {
     "duration": 0.070404,
     "end_time": "2025-01-13T03:46:08.660676",
     "exception": false,
     "start_time": "2025-01-13T03:46:08.590272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_last_day = pl.read_parquet(os.path.join(dirpath_initial_parquet2, '1698.parquet')) # Load the last day's Parquet file\n",
    "df_last_day = df_last_day.select([df_last_day[col].cast(pl.Float32).alias(col) for col in df_last_day.columns])\n",
    "    \n",
    "for feature in ALL_FEATURES:\n",
    "    stats = global_stats_dict[feature]\n",
    "    mean_val = stats[\"mean\"]\n",
    "    std_val = stats[\"std\"]\n",
    "    df_last_day = df_last_day.with_columns(\n",
    "        z_score(df_last_day[feature], mean_val, std_val).alias(feature)\n",
    "    )\n",
    "\n",
    "df_last_day = df_last_day.fill_null(0.0)  # Replace missing values with 0\n",
    "df_last_day = df_last_day.select([df_last_day[col].cast(pl.Float32).alias(col) for col in df_last_day.columns])\n",
    "df_last_day = df_last_day.sort(\"time_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "094cf3e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T03:46:08.714828Z",
     "iopub.status.busy": "2025-01-13T03:46:08.714556Z",
     "iopub.status.idle": "2025-01-13T03:46:08.737959Z",
     "shell.execute_reply": "2025-01-13T03:46:08.737249Z"
    },
    "papermill": {
     "duration": 0.051837,
     "end_time": "2025-01-13T03:46:08.739320",
     "exception": false,
     "start_time": "2025-01-13T03:46:08.687483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lags_ : pl.DataFrame | None = None\n",
    "\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n",
    "    # Use them as extra features, if you like.\n",
    "    global lags_, df_last_day, X_list, y_list, w_list\n",
    "    global predictor1, predictor2, predictor3\n",
    "    global refiner1, refiner2, refiner3\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    time_id = int(test[0, \"time_id\"])\n",
    "    date_id = int(test[0, \"date_id\"])\n",
    "    \n",
    "    not_scored = (test[\"is_scored\"] == False).all()\n",
    "    test = test.select([test[col].cast(pl.Float32).alias(col) for col in test.columns])\n",
    "\n",
    "    if lags is not None:\n",
    "        lags = lags.select([lags[col].cast(pl.Float32).alias(col) for col in lags.columns])\n",
    "        lags = lags.filter(\n",
    "            (lags['time_id'].is_not_null()) &\n",
    "            (lags['symbol_id'].is_not_null()) &\n",
    "            (lags['responder_6_lag_1'].is_not_null())\n",
    "        )\n",
    "\n",
    "        lags_ = lags\n",
    "        lags_ = lags_.drop(\"date_id\")\n",
    "        lags_ = lags_.rename({col: col.replace(\"_lag_1\", \"\") for col in lags_.columns if \"responder\" in col})    \n",
    "        lags_ = lags_.with_columns(z_score(lags_['time_id'], time_mean, time_std).alias('time_id'))\n",
    "        lags_ = lags_.select([lags_[col].cast(pl.Float32).alias(col) for col in lags_.columns])\n",
    "\n",
    "    # ========== Processing for time_id == 0 ==========\n",
    "    if time_id == 0:\n",
    "        print('-------- Date:', date_id)\n",
    "        \n",
    "        # ===== At the start of each date, finalize the previous day's data, split it, and add it to the list\n",
    "        merged = (df_last_day.join(lags_, on=[\"time_id\", \"symbol_id\"], how=\"inner\") if lags_ is not None else df_last_day)\n",
    "        merged = merged.fill_null(0.0)\n",
    "        merged = merged.sort(\"time_id\")\n",
    "        \n",
    "        unique_time_ids = merged['time_id'].unique()\n",
    "        num_time_ids = len(unique_time_ids)\n",
    "        split_size = num_time_ids // num_splits\n",
    "\n",
    "        for i in range(num_splits):\n",
    "            start = i * split_size\n",
    "            end = (i + 1) * split_size if i < num_splits - 1 else num_time_ids\n",
    "            if start >= num_time_ids:\n",
    "                break\n",
    "            subset_time_ids = unique_time_ids[start:end]\n",
    "            subset_df = merged.filter(pl.col(\"time_id\").is_in(subset_time_ids))\n",
    "            \n",
    "            X = torch.tensor(subset_df.select(ALL_FEATURES).to_numpy(), dtype=torch.float32)\n",
    "            y = torch.tensor(subset_df.select('responder_6').to_numpy(), dtype=torch.float32).squeeze()\n",
    "            w = torch.tensor(subset_df.select('weight').to_numpy(), dtype=torch.float32).squeeze() * weight_std + weight_mean\n",
    "\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            w_list.append(w)\n",
    "                \n",
    "    # ========== Process test batch ==========\n",
    "    for feature in ALL_FEATURES:\n",
    "        stats = global_stats_dict[feature]\n",
    "        mean_val = stats[\"mean\"]\n",
    "        std_val = stats[\"std\"]\n",
    "        test = test.with_columns(\n",
    "            z_score(test[feature], mean_val, std_val).alias(feature)\n",
    "        )\n",
    "\n",
    "    test = test.fill_null(0.0)\n",
    "    test = test.sort(\"time_id\")\n",
    "\n",
    "    # Collect today's feature data and set up for the next day\n",
    "    if time_id == 0:\n",
    "        # Assign directly for the first time_id\n",
    "        test = test.select([test[col].cast(pl.Float32).alias(col) for col in test.columns])\n",
    "        df_last_day = test\n",
    "        df_last_day = df_last_day.select([df_last_day[col].cast(pl.Float32).alias(col) for col in df_last_day.columns])\n",
    "    else:\n",
    "        # Append new_chunk to existing df_last_day\n",
    "        test = test.select([test[col].cast(pl.Float32).alias(col) for col in test.columns])\n",
    "        df_last_day = pl.concat([df_last_day, test], how=\"vertical\")\n",
    "        df_last_day = df_last_day.select([df_last_day[col].cast(pl.Float32).alias(col) for col in df_last_day.columns])\n",
    "\n",
    "    # ========== Training if have time ==========\n",
    "    if time_id == 0:\n",
    "        X_list_train, y_list_train, w_list_train = X_list[-num_splits:], y_list[-num_splits:], w_list[-num_splits:]\n",
    "        train_dataset = RMFDataset(X_list_train, y_list_train, w_list_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        predictor1.train()\n",
    "        for batch, (X_batch, y_batch, w_batch) in enumerate(train_loader, 1):\n",
    "            X_batch, y_batch, w_batch = X_batch[0].to(device), y_batch[0].to(device), w_batch[0].to(device)\n",
    "            \n",
    "            refiner1.zero_grad()\n",
    "            y_pred = predictor1(X_batch)\n",
    "            l2_reg = sum(param.pow(2).sum() for param in predictor1.parameters()) # L2 regularization\n",
    "            loss = weighted_r2_loss(y_batch, y_pred, w_batch) + lambda_reg * l2_reg\n",
    "            loss.backward()\n",
    "            refiner1.step()\n",
    "            \n",
    "        print(f\"Model 1 Fine tuned at date {date_id}\")\n",
    "        predictor1.eval()\n",
    "\n",
    "        predictor2.train()\n",
    "        for batch, (X_batch, y_batch, w_batch) in enumerate(train_loader, 1):\n",
    "            X_batch, y_batch, w_batch = X_batch[0].to(device), y_batch[0].to(device), w_batch[0].to(device)\n",
    "            \n",
    "            refiner2.zero_grad()\n",
    "            y_pred = predictor2(X_batch)\n",
    "            l2_reg = sum(param.pow(2).sum() for param in predictor2.parameters()) # L2 regularization\n",
    "            loss = weighted_r2_loss(y_batch, y_pred, w_batch) + lambda_reg * l2_reg\n",
    "            loss.backward()\n",
    "            refiner2.step()\n",
    "            \n",
    "        print(f\"Model 2 Fine tuned at date {date_id}\")\n",
    "        predictor2.eval()\n",
    "\n",
    "        predictor3.train()\n",
    "        for batch, (X_batch, y_batch, w_batch) in enumerate(train_loader, 1):\n",
    "            X_batch, y_batch, w_batch = X_batch[0].to(device), y_batch[0].to(device), w_batch[0].to(device)\n",
    "            \n",
    "            refiner3.zero_grad()\n",
    "            y_pred = predictor3(X_batch)\n",
    "            l2_reg = sum(param.pow(2).sum() for param in predictor3.parameters()) # L2 regularization\n",
    "            loss = weighted_r2_loss(y_batch, y_pred, w_batch) + lambda_reg * l2_reg\n",
    "            loss.backward()\n",
    "            refiner3.step()\n",
    "            \n",
    "        print(f\"Model 3 Fine tuned at date {date_id}\")\n",
    "        predictor3.eval()\n",
    "            \n",
    "    # ========== Inference ==========    \n",
    "    if not_scored: # no need to score\n",
    "        predictions = test.select(\n",
    "            'row_id',\n",
    "            pl.lit(0.0).alias('responder_6'),\n",
    "        )\n",
    "        \n",
    "        predictions = predictions.with_columns(\n",
    "            pl.col(\"row_id\").cast(pl.Int64).alias(\"row_id\"),  # Ensure 'row_id' is Int64\n",
    "            pl.col(\"responder_6\").cast(pl.Float64).alias(\"responder_6\")  # Ensure 'responder_6' is Float64\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        predictor1.eval()\n",
    "        predictor2.eval()\n",
    "        predictor3.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test = torch.tensor(test.select(ALL_FEATURES).to_numpy(), dtype=torch.float32).to(device)\n",
    "            y_test1 = predictor1(X_test).detach().cpu().numpy().flatten()\n",
    "            y_test2 = predictor2(X_test).detach().cpu().numpy().flatten()\n",
    "            y_test3 = predictor3(X_test).detach().cpu().numpy().flatten()\n",
    "\n",
    "        y_test = (y_test1 + y_test2 + y_test3) / 3.0        \n",
    "        y_test = np.nan_to_num(y_test, nan=0.0, posinf=0.0, neginf=-0.0)\n",
    "        \n",
    "        # Create predictions DataFrame\n",
    "        predictions = pl.DataFrame({\n",
    "            \"row_id\": test[\"row_id\"].to_numpy(),  # Convert 'row_id' to numpy for compatibility\n",
    "            \"responder_6\": y_test.astype('float64')    # Use flattened dummy output\n",
    "        })\n",
    "        \n",
    "        # predictions = predictions.with_columns(pl.col(\"responder_6\").fill_null(0.0).alias(\"responder_6\"))\n",
    "        predictions = predictions.with_columns(\n",
    "            pl.col(\"row_id\").cast(pl.Int64).alias(\"row_id\"),  # Ensure 'row_id' is Int64\n",
    "            pl.col(\"responder_6\").cast(pl.Float64).alias(\"responder_6\")  # Ensure 'responder_6' is Float64\n",
    "        )\n",
    "    \n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "        \n",
    "    assert len(predictions) == len(test) # Confirm has as many rows as the test data.\n",
    "        \n",
    "    # ========== Post Assertion ==========\n",
    "    # if predictions[\"row_id\"].null_count() > 0 or predictions[\"responder_6\"].null_count() > 0:\n",
    "    #     raise AssertionError(\n",
    "    #         f\"Found {total_nulls} null values in the predictions DataFrame for date_id: {date_id}, time_id: {time_id}.\"\n",
    "    #     )\n",
    "\n",
    "    # nan_present = predictions['responder_6'].is_nan().any()\n",
    "    # if nan_present:\n",
    "    #     raise AssertionError(\n",
    "    #         f\"Found NaN values in the 'responder_6' column of the predictions DataFrame for date_id: {date_id}, time_id: {time_id}.\"\n",
    "    #     )\n",
    "    \n",
    "    # print(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "444eeb6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T03:46:08.794373Z",
     "iopub.status.busy": "2025-01-13T03:46:08.794075Z",
     "iopub.status.idle": "2025-01-13T03:51:48.988839Z",
     "shell.execute_reply": "2025-01-13T03:51:48.987846Z"
    },
    "papermill": {
     "duration": 340.224273,
     "end_time": "2025-01-13T03:51:48.990520",
     "exception": false,
     "start_time": "2025-01-13T03:46:08.766247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Date: 0\n",
      "Model 1 Fine tuned at date 0\n",
      "Model 2 Fine tuned at date 0\n",
      "Model 3 Fine tuned at date 0\n",
      "-------- Date: 1\n",
      "Model 1 Fine tuned at date 1\n",
      "Model 2 Fine tuned at date 1\n",
      "Model 3 Fine tuned at date 1\n",
      "-------- Date: 2\n",
      "Model 1 Fine tuned at date 2\n",
      "Model 2 Fine tuned at date 2\n",
      "Model 3 Fine tuned at date 2\n",
      "-------- Date: 3\n",
      "Model 1 Fine tuned at date 3\n",
      "Model 2 Fine tuned at date 3\n",
      "Model 3 Fine tuned at date 3\n",
      "-------- Date: 4\n",
      "Model 1 Fine tuned at date 4\n",
      "Model 2 Fine tuned at date 4\n",
      "Model 3 Fine tuned at date 4\n",
      "-------- Date: 5\n",
      "Model 1 Fine tuned at date 5\n",
      "Model 2 Fine tuned at date 5\n",
      "Model 3 Fine tuned at date 5\n",
      "-------- Date: 6\n",
      "Model 1 Fine tuned at date 6\n",
      "Model 2 Fine tuned at date 6\n",
      "Model 3 Fine tuned at date 6\n",
      "-------- Date: 7\n",
      "Model 1 Fine tuned at date 7\n",
      "Model 2 Fine tuned at date 7\n",
      "Model 3 Fine tuned at date 7\n",
      "-------- Date: 8\n",
      "Model 1 Fine tuned at date 8\n",
      "Model 2 Fine tuned at date 8\n",
      "Model 3 Fine tuned at date 8\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            # '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
    "            # '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
    "            '/kaggle/input/synthetic-data/synthetic_test.parquet',\n",
    "            '/kaggle/input/synthetic-data/synthetic_lag.parquet',\n",
    "            \n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    },
    {
     "datasetId": 6416806,
     "sourceId": 10361211,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6444133,
     "sourceId": 10400324,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6444185,
     "sourceId": 10400384,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6414805,
     "sourceId": 10446037,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 940.150242,
   "end_time": "2025-01-13T03:51:51.355707",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-13T03:36:11.205465",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
