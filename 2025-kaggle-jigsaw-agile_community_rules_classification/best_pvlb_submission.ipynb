{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425ae863",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-23T09:39:49.193621Z",
     "iopub.status.busy": "2025-10-23T09:39:49.192965Z",
     "iopub.status.idle": "2025-10-23T09:39:49.223798Z",
     "shell.execute_reply": "2025-10-23T09:39:49.222808Z"
    },
    "papermill": {
     "duration": 0.040235,
     "end_time": "2025-10-23T09:39:49.225524",
     "exception": false,
     "start_time": "2025-10-23T09:39:49.185289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_on_gpu_0_gemma3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_on_gpu_0_gemma3.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "MODEL_NAME = \"/kaggle/input/llm-gemma3/gemma-3-transformers-gemma-3-1b-it-v1\"\n",
    "DATA_PATH_TRAIN = \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"\n",
    "DATA_PATH_TEST = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_OUTPUT_PATH = f\"./lora_checkpoint_{timestamp}\"\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCH = 2\n",
    "LR = 1e-4\n",
    "TRAIN_BS = 1\n",
    "GRAD_ACC_NUM = 8\n",
    "SEED = 42\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_POS_EXAMPLES = 3\n",
    "NUM_NEG_EXAMPLES = 3\n",
    "\n",
    "# Inference parameters\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset Building\n",
    "# ============================================================================\n",
    "def build_training_dataset_dict(data_path_train: str, data_path_test: str, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Build training dataset from:\n",
    "    1. Examples from test.csv (positive_example_1, positive_example_2, negative_example_1, negative_example_2)\n",
    "    2. Examples AND nonexamples from train.csv\n",
    "    \n",
    "    Returns dict with structure:\n",
    "    {\n",
    "        rule: {\n",
    "            'positive_example': list of texts that violate the rule,\n",
    "            'negative_example': list of texts that don't violate the rule\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    train_df = pd.read_csv(data_path_train)\n",
    "    test_df = pd.read_csv(data_path_test)\n",
    "    \n",
    "    dataset_dict = defaultdict(lambda: {\n",
    "        'positive_example': set(),\n",
    "        'negative_example': set()\n",
    "    })\n",
    "    \n",
    "    # Process train.csv - both examples and nonexamples\n",
    "    print(\"Processing train.csv...\")\n",
    "    for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "        rule = row['rule']\n",
    "        \n",
    "        # Add body with its rule_violation label\n",
    "        body = str(row['body']).strip() if pd.notna(row['body']) else \"\"\n",
    "        if body:\n",
    "            if row['rule_violation'] == 1:\n",
    "                dataset_dict[rule]['positive_example'].add(body)\n",
    "            else:\n",
    "                dataset_dict[rule]['negative_example'].add(body)\n",
    "        \n",
    "        # Add explicit examples\n",
    "        for i in range(1, 3):\n",
    "            pos_col = f'positive_example_{i}'\n",
    "            if pos_col in train_df.columns:\n",
    "                text = str(row[pos_col]).strip() if pd.notna(row[pos_col]) else \"\"\n",
    "                if text:\n",
    "                    dataset_dict[rule]['positive_example'].add(text)\n",
    "            \n",
    "            neg_col = f'negative_example_{i}'\n",
    "            if neg_col in train_df.columns:\n",
    "                text = str(row[neg_col]).strip() if pd.notna(row[neg_col]) else \"\"\n",
    "                if text:\n",
    "                    dataset_dict[rule]['negative_example'].add(text)\n",
    "    \n",
    "    # Process test.csv - only explicit examples\n",
    "    print(\"Processing test.csv examples...\")\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        rule = row['rule']\n",
    "        \n",
    "        for i in range(1, 3):\n",
    "            pos_col = f'positive_example_{i}'\n",
    "            if pos_col in test_df.columns:\n",
    "                text = str(row[pos_col]).strip() if pd.notna(row[pos_col]) else \"\"\n",
    "                if text:\n",
    "                    dataset_dict[rule]['positive_example'].add(text)\n",
    "            \n",
    "            neg_col = f'negative_example_{i}'\n",
    "            if neg_col in test_df.columns:\n",
    "                text = str(row[neg_col]).strip() if pd.notna(row[neg_col]) else \"\"\n",
    "                if text:\n",
    "                    dataset_dict[rule]['negative_example'].add(text)\n",
    "    \n",
    "    # Convert sets to lists\n",
    "    result_dict = {}\n",
    "    for rule, value in dataset_dict.items():\n",
    "        result_dict[rule] = {\n",
    "            'positive_example': list(value['positive_example']),\n",
    "            'negative_example': list(value['negative_example'])\n",
    "        }\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def build_inference_data(data_path_test: str):\n",
    "    \"\"\"\n",
    "    Build inference data from test.csv body column (nonexamples)\n",
    "    \n",
    "    Returns list of dicts with:\n",
    "    [\n",
    "        {\n",
    "            'row_id': row_id,\n",
    "            'rule': rule text,\n",
    "            'text': body text\n",
    "        }\n",
    "    ]\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(data_path_test)\n",
    "    \n",
    "    inference_data = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        body = str(row['body']).strip() if pd.notna(row['body']) else \"\"\n",
    "        if body:\n",
    "            inference_data.append({\n",
    "                'row_id': row['row_id'],\n",
    "                'rule': row['rule'],\n",
    "                'text': body\n",
    "            })\n",
    "    \n",
    "    return inference_data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Training Dataset\n",
    "# ============================================================================\n",
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Training dataset that creates prompts with examples and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset_dict,\n",
    "        tokenizer,\n",
    "        num_pos_examples: int = 2,\n",
    "        num_neg_examples: int = 2,\n",
    "        sys_prompt: str = \"You are a content moderator for Reddit. Your task is to determine if comments violate community rules. Use the examples as guidance. Answer only 'Yes' if it violates the rule, or 'No' if it doesn't.\",\n",
    "        seed: int = None\n",
    "    ):\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_pos_examples = num_pos_examples\n",
    "        self.num_neg_examples = num_neg_examples\n",
    "        self.sys_prompt = sys_prompt\n",
    "        \n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        \n",
    "        # Create training examples\n",
    "        self.examples = []\n",
    "        \n",
    "        for rule, categories in dataset_dict.items():\n",
    "            pos_examples = categories['positive_example']\n",
    "            neg_examples = categories['negative_example']\n",
    "            \n",
    "            # Add positive examples as training targets\n",
    "            for text in pos_examples:\n",
    "                self.examples.append({\n",
    "                    'rule': rule,\n",
    "                    'text': text,\n",
    "                    'label': 1,\n",
    "                    'pos_pool': [p for p in pos_examples if p != text],\n",
    "                    'neg_pool': neg_examples\n",
    "                })\n",
    "            \n",
    "            # Add negative examples as training targets\n",
    "            for text in neg_examples:\n",
    "                self.examples.append({\n",
    "                    'rule': rule,\n",
    "                    'text': text,\n",
    "                    'label': 0,\n",
    "                    'pos_pool': pos_examples,\n",
    "                    'neg_pool': [n for n in neg_examples if n != text]\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def _build_prompt_text(self, example, sampled_pos_examples, sampled_neg_examples):\n",
    "        \"\"\"Build prompt with rule, examples, and text to classify\"\"\"\n",
    "        prompt_parts = [\n",
    "            self.sys_prompt,\n",
    "            \"\",\n",
    "            f\"Rule: \\\"{example['rule']}\\\"\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        if sampled_pos_examples:\n",
    "            prompt_parts.append(\"Here are examples of comments that VIOLATE this rule:\")\n",
    "            for i, pos_ex in enumerate(sampled_pos_examples, 1):\n",
    "                prompt_parts.append(f\"{i}. {pos_ex}\")\n",
    "            prompt_parts.append(\"\")\n",
    "        \n",
    "        if sampled_neg_examples:\n",
    "            prompt_parts.append(\"Here are examples of comments that DO NOT violate this rule:\")\n",
    "            for i, neg_ex in enumerate(sampled_neg_examples, 1):\n",
    "                prompt_parts.append(f\"{i}. {neg_ex}\")\n",
    "            prompt_parts.append(\"\")\n",
    "        \n",
    "        prompt_parts.extend([\n",
    "            \"Now, evaluate this comment:\",\n",
    "            f\"\\\"{example['text']}\\\"\",\n",
    "            \"\",\n",
    "            \"Does it violate the rule?\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(prompt_parts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Sample random examples\n",
    "        num_pos_to_sample = min(self.num_pos_examples, len(example['pos_pool']))\n",
    "        sampled_pos_examples = random.sample(example['pos_pool'], num_pos_to_sample) if num_pos_to_sample > 0 else []\n",
    "        \n",
    "        num_neg_to_sample = min(self.num_neg_examples, len(example['neg_pool']))\n",
    "        sampled_neg_examples = random.sample(example['neg_pool'], num_neg_to_sample) if num_neg_to_sample > 0 else []\n",
    "        \n",
    "        # Build prompt\n",
    "        user_content = self._build_prompt_text(example, sampled_pos_examples, sampled_neg_examples)\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        \n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "        \n",
    "        label_text = \" Yes\" if example['label'] == 1 else \" No\"\n",
    "        full_text = prompt + label_text\n",
    "        \n",
    "        tokenized = self.tokenizer(full_text, add_special_tokens=False, truncation=False)\n",
    "        \n",
    "        return {\"input_ids\": tokenized[\"input_ids\"]}\n",
    "\n",
    "\n",
    "class ClassifyDataset(Dataset):\n",
    "    \"\"\"Wrapper for Trainer compatibility\"\"\"\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.base_dataset[index]\n",
    "        return {\"input_ids\": item[\"input_ids\"]}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Inference Functions\n",
    "# ============================================================================\n",
    "def get_yes_no_probabilities(model, tokenizer, prompts):\n",
    "    \"\"\"Get probability scores for Yes/No tokens\"\"\"\n",
    "    \n",
    "    yes_tokens = tokenizer.encode(\" Yes\", add_special_tokens=False)\n",
    "    no_tokens = tokenizer.encode(\" No\", add_special_tokens=False)\n",
    "    \n",
    "    yes_token_id = yes_tokens[0]\n",
    "    no_token_id = no_tokens[0]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    \n",
    "    yes_logits = last_token_logits[:, yes_token_id]\n",
    "    no_logits = last_token_logits[:, no_token_id]\n",
    "    \n",
    "    yes_no_logits = torch.stack([no_logits, yes_logits], dim=1)\n",
    "    probabilities = F.softmax(yes_no_logits, dim=1)\n",
    "    \n",
    "    yes_probs = probabilities[:, 1].cpu().numpy()\n",
    "    return yes_probs\n",
    "\n",
    "\n",
    "def build_inference_prompt(rule, text, pos_examples, neg_examples, sys_prompt, tokenizer, num_pos=2, num_neg=2):\n",
    "    \"\"\"Build prompt for inference\"\"\"\n",
    "    # Sample examples\n",
    "    sampled_pos = random.sample(pos_examples, min(num_pos, len(pos_examples))) if pos_examples else []\n",
    "    sampled_neg = random.sample(neg_examples, min(num_neg, len(neg_examples))) if neg_examples else []\n",
    "    \n",
    "    prompt_parts = [\n",
    "        sys_prompt,\n",
    "        \"\",\n",
    "        f\"Rule: \\\"{rule}\\\"\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    if sampled_pos:\n",
    "        prompt_parts.append(\"Here are examples of comments that VIOLATE this rule:\")\n",
    "        for i, pos_ex in enumerate(sampled_pos, 1):\n",
    "            prompt_parts.append(f\"{i}. {pos_ex}\")\n",
    "        prompt_parts.append(\"\")\n",
    "    \n",
    "    if sampled_neg:\n",
    "        prompt_parts.append(\"Here are examples of comments that DO NOT violate this rule:\")\n",
    "        for i, neg_ex in enumerate(sampled_neg, 1):\n",
    "            prompt_parts.append(f\"{i}. {neg_ex}\")\n",
    "        prompt_parts.append(\"\")\n",
    "    \n",
    "    prompt_parts.extend([\n",
    "        \"Now, evaluate this comment:\",\n",
    "        f\"\\\"{text}\\\"\",\n",
    "        \"\",\n",
    "        \"Does it violate the rule?\"\n",
    "    ])\n",
    "    \n",
    "    user_content = \"\\n\".join(prompt_parts)\n",
    "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Script\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Gemma-3 Training and Inference Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 1: Build dataset\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n[Step 1/5] Building training dataset...\")\n",
    "dataset_dict = build_training_dataset_dict(DATA_PATH_TRAIN, DATA_PATH_TEST, seed=SEED)\n",
    "print(f\"Total rules: {len(dataset_dict)}\")\n",
    "\n",
    "total_pos = sum(len(v['positive_example']) for v in dataset_dict.values())\n",
    "total_neg = sum(len(v['negative_example']) for v in dataset_dict.values())\n",
    "print(f\"Total positive examples: {total_pos}\")\n",
    "print(f\"Total negative examples: {total_neg}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 2: Initialize tokenizer\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n[Step 2/5] Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 3: Create training dataset\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n[Step 3/5] Creating training dataset...\")\n",
    "train_base_dataset = TrainingDataset(\n",
    "    dataset_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    num_pos_examples=NUM_POS_EXAMPLES,\n",
    "    num_neg_examples=NUM_NEG_EXAMPLES,\n",
    "    seed=SEED\n",
    ")\n",
    "train_dataset = ClassifyDataset(train_base_dataset)\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\"Does it violate the rule?\", tokenizer=tokenizer)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 4: Train model\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n[Step 4/5] Training model...\")\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation='eager'\n",
    ")\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    bias='none',\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_PATH,\n",
    "    logging_steps=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=EPOCH,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    bf16=is_torch_bf16_gpu_available(),\n",
    "    fp16=not is_torch_bf16_gpu_available(),\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC_NUM,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    group_by_length=False,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer_output = trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final training loss: {trainer_output.training_loss:.4f}\")\n",
    "print(f\"Saving model to: {MODEL_OUTPUT_PATH}\")\n",
    "trainer.save_model(MODEL_OUTPUT_PATH)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 5: Inference\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n[Step 5/5] Running inference on test.csv...\")\n",
    "\n",
    "# Load inference data\n",
    "inference_data = build_inference_data(DATA_PATH_TEST)\n",
    "print(f\"Total inference samples: {len(inference_data)}\")\n",
    "\n",
    "# Load trained model\n",
    "print(\"Loading trained model...\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ").to(device)\n",
    "\n",
    "trained_model = PeftModel.from_pretrained(base_model, MODEL_OUTPUT_PATH)\n",
    "trained_model.eval()\n",
    "\n",
    "# Prepare prompts\n",
    "print(\"Preparing prompts...\")\n",
    "sys_prompt = \"You are a content moderator for Reddit. Your task is to determine if comments violate community rules. Use the examples as guidance. Answer only 'Yes' if it violates the rule, or 'No' if it doesn't.\"\n",
    "\n",
    "all_prompts = []\n",
    "all_row_ids = []\n",
    "\n",
    "for item in tqdm(inference_data):\n",
    "    rule = item['rule']\n",
    "    text = item['text']\n",
    "    row_id = item['row_id']\n",
    "    \n",
    "    # Get examples for this rule\n",
    "    pos_examples = dataset_dict.get(rule, {}).get('positive_example', [])\n",
    "    neg_examples = dataset_dict.get(rule, {}).get('negative_example', [])\n",
    "    \n",
    "    prompt = build_inference_prompt(\n",
    "        rule, text, pos_examples, neg_examples, \n",
    "        sys_prompt, tokenizer, NUM_POS_EXAMPLES, NUM_NEG_EXAMPLES\n",
    "    )\n",
    "    \n",
    "    all_prompts.append(prompt)\n",
    "    all_row_ids.append(row_id)\n",
    "\n",
    "# Run inference in batches\n",
    "print(\"Running inference...\")\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(all_prompts), BATCH_SIZE)):\n",
    "        batch_prompts = all_prompts[i:i + BATCH_SIZE]\n",
    "        batch_probs = get_yes_no_probabilities(trained_model, tokenizer, batch_prompts)\n",
    "        all_probabilities.extend(batch_probs)\n",
    "\n",
    "# Create submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'row_id': all_row_ids,\n",
    "    'rule_violation': all_probabilities\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission1.csv', index=False)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Pipeline complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Submission saved to: submission1.csv\")\n",
    "print(f\"Total predictions: {len(submission_df)}\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef814b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T09:39:49.235131Z",
     "iopub.status.busy": "2025-10-23T09:39:49.234900Z",
     "iopub.status.idle": "2025-10-23T09:39:49.249310Z",
     "shell.execute_reply": "2025-10-23T09:39:49.248411Z"
    },
    "papermill": {
     "duration": 0.020301,
     "end_time": "2025-10-23T09:39:49.251007",
     "exception": false,
     "start_time": "2025-10-23T09:39:49.230706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_on_gpu_0_bert.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_on_gpu_0_bert.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "class Config:\n",
    "    model_name_or_path = r\"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\"\n",
    "    num_train_epochs = 3\n",
    "    learning_rate = 2e-5\n",
    "    per_device_train_batch_size = 24\n",
    "    per_device_eval_batch_size = 64\n",
    "    data_path = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "    max_length = 512\n",
    "    warmup_ratio = 0.1\n",
    "    weight_decay = 0.01\n",
    "    seed = 42\n",
    "    logging_steps = 50\n",
    "    eval_ratio = 0.1  # Use 10% of training data for validation\n",
    "\n",
    "# Dataset Classes\n",
    "class JigsawBertDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that tokenizes during initialization.\n",
    "    Includes both example and nonexample texts for training.\n",
    "    \n",
    "    Combines rule and text with [SEP] token and returns tokenized tensors.\n",
    "    \n",
    "    Special labeling:\n",
    "    - Positive examples/nonexamples: label = 1\n",
    "    - Negative examples containing \"http\": label = 0.25\n",
    "    - Negative examples without \"http\": label = 0\n",
    "    - Negative nonexamples (from body): always label = 0\n",
    "    \n",
    "    Args:\n",
    "        dataset_dict: Dictionary with rule keys and category keys\n",
    "        tokenizer: Tokenizer to use for encoding texts\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dict: dict, tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels = []\n",
    "        \n",
    "        # Flatten the dataset_dict and create input texts\n",
    "        input_texts = []\n",
    "        for rule, categories in dataset_dict.items():\n",
    "            # Add positive examples (label=1)\n",
    "            for text in categories['positive_example']:\n",
    "                input_text = f\"{rule}[SEP]{text}\"\n",
    "                input_texts.append(input_text)\n",
    "                self.labels.append(1)\n",
    "\n",
    "            # Add positive nonexamples (label=1)\n",
    "            for text in categories['positive_nonexample']:\n",
    "                input_text = f\"{rule}[SEP]{text}\"\n",
    "                input_texts.append(input_text)\n",
    "                self.labels.append(1)\n",
    "            \n",
    "            # Add negative examples (label=0 or 0.25 if contains http)\n",
    "            for text in categories['negative_example']:\n",
    "                input_text = f\"{rule}[SEP]{text}\"\n",
    "                input_texts.append(input_text)\n",
    "                # Check if text contains \"http\"\n",
    "                label = 0.25 if 'http' in text.lower() else 0\n",
    "                self.labels.append(label)\n",
    "            \n",
    "            # Add negative nonexamples (always label=0, from body text)\n",
    "            for text in categories['negative_nonexample']:\n",
    "                input_text = f\"{rule}[SEP]{text}\"\n",
    "                input_texts.append(input_text)\n",
    "                self.labels.append(0)\n",
    "        \n",
    "        # Tokenize all texts at once\n",
    "        self.encodings = tokenizer(\n",
    "            input_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "class InferenceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for inference with only body texts.\n",
    "    \"\"\"\n",
    "    def __init__(self, inference_data: dict, tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.row_ids = []\n",
    "        \n",
    "        input_texts = []\n",
    "        for row_id, data in inference_data.items():\n",
    "            rule = data['rule']\n",
    "            text = data['body']\n",
    "            \n",
    "            input_text = f\"{rule}[SEP]{text}\"\n",
    "            input_texts.append(input_text)\n",
    "            self.row_ids.append(row_id)\n",
    "        \n",
    "        self.encodings = tokenizer(\n",
    "            input_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.row_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "\n",
    "# Data Building Functions\n",
    "def build_training_dataset(\n",
    "    data_path_train: str,\n",
    "    data_path_test: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Build dataset for training that includes:\n",
    "    - All examples and non-examples from train.csv\n",
    "    - All examples from test.csv\n",
    "    \n",
    "    Examples are grouped by rule only (subreddit is ignored).\n",
    "    Duplicates are removed across all subreddits for the same rule.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with rule keys\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(data_path_train)\n",
    "    test_df = pd.read_csv(data_path_test)\n",
    "    \n",
    "    dataset_dict = defaultdict(lambda: {\n",
    "        'positive_example': set(),\n",
    "        'positive_nonexample': set(),\n",
    "        'negative_example': set(),\n",
    "        'negative_nonexample': set()\n",
    "    })\n",
    "    \n",
    "    # Process train.csv - add all examples and non-examples\n",
    "    for _, row in tqdm(train_df.iterrows(), total=train_df.shape[0], desc=\"Processing train.csv\"):\n",
    "        rule = row['rule']\n",
    "        \n",
    "        # Add body with its rule_violation label as nonexample\n",
    "        body = str(row['body']).strip() if pd.notna(row['body']) else \"\"\n",
    "        if body and len(body) > 0:\n",
    "            label_key = 'positive_nonexample' if row['rule_violation'] == 1 else 'negative_nonexample'\n",
    "            dataset_dict[rule][label_key].add(body)\n",
    "        \n",
    "        # Add positive examples (1, 2)\n",
    "        for i in range(1, 3):\n",
    "            col_name = f'positive_example_{i}'\n",
    "            if col_name in train_df.columns:\n",
    "                text = str(row[col_name]).strip() if pd.notna(row[col_name]) else \"\"\n",
    "                if text and len(text) > 0:\n",
    "                    dataset_dict[rule]['positive_example'].add(text)\n",
    "        \n",
    "        # Add negative examples (1, 2)\n",
    "        for i in range(1, 3):\n",
    "            col_name = f'negative_example_{i}'\n",
    "            if col_name in train_df.columns:\n",
    "                text = str(row[col_name]).strip() if pd.notna(row[col_name]) else \"\"\n",
    "                if text and len(text) > 0:\n",
    "                    dataset_dict[rule]['negative_example'].add(text)\n",
    "    \n",
    "    # Process test.csv - add only explicit examples\n",
    "    for _, row in tqdm(test_df.iterrows(), total=test_df.shape[0], desc=\"Processing test.csv\"):\n",
    "        rule = row['rule']\n",
    "        \n",
    "        # Add positive examples (1, 2)\n",
    "        for i in range(1, 3):\n",
    "            col_name = f'positive_example_{i}'\n",
    "            if col_name in test_df.columns:\n",
    "                text = str(row[col_name]).strip() if pd.notna(row[col_name]) else \"\"\n",
    "                if text and len(text) > 0:\n",
    "                    dataset_dict[rule]['positive_example'].add(text)\n",
    "        \n",
    "        # Add negative examples (1, 2)\n",
    "        for i in range(1, 3):\n",
    "            col_name = f'negative_example_{i}'\n",
    "            if col_name in test_df.columns:\n",
    "                text = str(row[col_name]).strip() if pd.notna(row[col_name]) else \"\"\n",
    "                if text and len(text) > 0:\n",
    "                    dataset_dict[rule]['negative_example'].add(text)\n",
    "    \n",
    "    # Convert defaultdict to regular dict (sets are already deduplicated)\n",
    "    result_dict = dict(dataset_dict)\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def build_inference_dataset(data_path_test: str) -> dict:\n",
    "    \"\"\"\n",
    "    Build dataset for inference that includes only non-examples from test.csv\n",
    "    (body column with row_id for tracking).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with row_id as key and (rule, body) as value\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(data_path_test)\n",
    "    \n",
    "    inference_data = {}\n",
    "    for _, row in tqdm(test_df.iterrows(), total=test_df.shape[0], desc=\"Processing test.csv for inference\"):\n",
    "        row_id = row['row_id']\n",
    "        rule = row['rule']\n",
    "        body = str(row['body']).strip() if pd.notna(row['body']) else \"\"\n",
    "        \n",
    "        if body and len(body) > 0:\n",
    "            inference_data[row_id] = {\n",
    "                'rule': rule,\n",
    "                'body': body\n",
    "            }\n",
    "    \n",
    "    return inference_data\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"Evaluate model on validation set\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1)[:, 1]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    metrics = {\n",
    "        'auc': roc_auc_score(all_labels, all_probs),\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def generate_predictions(model, inference_loader, device):\n",
    "    \"\"\"Generate predictions for inference data\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(inference_loader, desc=\"Generating predictions\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1)[:, 1]\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_probs)\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"Main training function with manual optimizer and scheduler\"\"\"\n",
    "    cfg = Config()\n",
    "    set_seed(cfg.seed)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Training started at {datetime.now()}\")\n",
    "    print(f\"Model: {cfg.model_name_or_path}\")\n",
    "    print(f\"Epochs: {cfg.num_train_epochs}, Batch size: {cfg.per_device_train_batch_size}, LR: {cfg.learning_rate}\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading datasets...\")\n",
    "    train_data_dict = build_training_dataset(\n",
    "        data_path_train=f\"{cfg.data_path}/train.csv\",\n",
    "        data_path_test=f\"{cfg.data_path}/test.csv\"\n",
    "    )\n",
    "    \n",
    "    # Print statistics about rules and examples\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Number of unique rules: {len(train_data_dict)}\")\n",
    "    total_examples = sum(\n",
    "        len(cats['positive_example']) + len(cats['positive_nonexample']) +\n",
    "        len(cats['negative_example']) + len(cats['negative_nonexample'])\n",
    "        for cats in train_data_dict.values()\n",
    "    )\n",
    "    print(f\"Total examples across all rules: {total_examples}\")\n",
    "    \n",
    "    # FIXED: Suppress tokenizer conversion warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*sentencepiece tokenizer.*\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name_or_path, use_fast=False)\n",
    "    \n",
    "    train_dataset = JigsawBertDataset(train_data_dict, tokenizer, max_length=cfg.max_length)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_size = len(train_dataset)\n",
    "    print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.per_device_train_batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model_name_or_path, \n",
    "        num_labels=2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(), \n",
    "        lr=cfg.learning_rate, \n",
    "        weight_decay=cfg.weight_decay\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_loader) * cfg.num_train_epochs\n",
    "    warmup_steps = int(total_steps * cfg.warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        warmup_steps, \n",
    "        total_steps\n",
    "    )\n",
    "    \n",
    "    print(f\"Total steps: {total_steps}\")\n",
    "    print(f\"Warmup steps: {warmup_steps}\\n\")\n",
    "    \n",
    "    # Load inference data for predictions at each epoch\n",
    "    print(\"Loading inference dataset...\")\n",
    "    inference_data = build_inference_dataset(f\"{cfg.data_path}/test.csv\")\n",
    "    inference_dataset = InferenceDataset(\n",
    "        inference_data, \n",
    "        tokenizer, \n",
    "        max_length=cfg.max_length\n",
    "    )\n",
    "    \n",
    "    inference_loader = torch.utils.data.DataLoader(\n",
    "        inference_dataset,\n",
    "        batch_size=cfg.per_device_eval_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Inference dataset: {len(inference_dataset)} samples\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"=\"*60)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for epoch in range(cfg.num_train_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{cfg.num_train_epochs}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if (step + 1) % cfg.logging_steps == 0:\n",
    "                avg_loss = total_loss / (step + 1)\n",
    "                print(f\"Step {step + 1}, Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        avg_epoch_loss = total_loss / len(train_loader)\n",
    "        print(f\"Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        # Generate predictions on inference set\n",
    "        print(\"\\nGenerating predictions on inference set...\")\n",
    "        probs = generate_predictions(model, inference_loader, device)\n",
    "        \n",
    "        # Create submission DataFrame\n",
    "        submission_df = pd.DataFrame({\n",
    "            'row_id': inference_dataset.row_ids,\n",
    "            'rule_violation': probs\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nSubmission results for Epoch {epoch + 1}:\")\n",
    "        print(f\"  Shape: {submission_df.shape}\")\n",
    "        print(f\"  Mean probability: {probs.mean():.4f}\")\n",
    "        print(f\"  Min probability: {probs.min():.4f}\")\n",
    "        print(f\"  Max probability: {probs.max():.4f}\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(submission_df.head())\n",
    "        \n",
    "    # Save submission for this epoch\n",
    "    submission_df.to_csv('submission2.csv', index=False)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ceed52b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T09:39:49.257991Z",
     "iopub.status.busy": "2025-10-23T09:39:49.257811Z",
     "iopub.status.idle": "2025-10-23T09:39:49.273262Z",
     "shell.execute_reply": "2025-10-23T09:39:49.272343Z"
    },
    "papermill": {
     "duration": 0.021133,
     "end_time": "2025-10-23T09:39:49.274963",
     "exception": false,
     "start_time": "2025-10-23T09:39:49.253830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_on_gpu_1_qwen3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_on_gpu_1_qwen3.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "MODEL_NAME = \"/kaggle/input/qwen-3/transformers/1.7b-gptq-int8/1\"\n",
    "DATA_PATH_TRAIN = \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"\n",
    "DATA_PATH_TEST = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_OUTPUT_PATH = f\"./lora_checkpoint_{timestamp}\"\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCH = 2\n",
    "LR = 1e-4\n",
    "TRAIN_BS = 1\n",
    "GRAD_ACC_NUM = 8\n",
    "SEED = 42\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_POS_EXAMPLES = 2\n",
    "NUM_NEG_EXAMPLES = 2\n",
    "\n",
    "# Inference parameters\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset Building\n",
    "# ============================================================================\n",
    "def build_training_dataset_dict(data_path_train: str, data_path_test: str, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Build training dataset from:\n",
    "    1. Examples from test.csv (positive_example_1, positive_example_2, negative_example_1, negative_example_2)\n",
    "    2. Examples AND nonexamples from train.csv\n",
    "    \n",
    "    Returns dict with structure:\n",
    "    {\n",
    "        rule: {\n",
    "            'positive_example': list of texts that violate the rule,\n",
    "            'negative_example': list of texts that don't violate the rule\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    train_df = pd.read_csv(data_path_train)\n",
    "    test_df = pd.read_csv(data_path_test)\n",
    "    \n",
    "    dataset_dict = defaultdict(lambda: {\n",
    "        'positive_example': set(),\n",
    "        'negative_example': set()\n",
    "    })\n",
    "    \n",
    "    # Process train.csv - both examples and nonexamples\n",
    "    print(\"Processing train.csv...\")\n",
    "    for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "        rule = row['rule']\n",
    "        \n",
    "        # Add body with its rule_violation label\n",
    "        body = str(row['body']).strip() if pd.notna(row['body']) else \"\"\n",
    "        if body:\n",
    "            if row['rule_violation'] == 1:\n",
    "                dataset_dict[rule]['positive_example'].add(body)\n",
    "            else:\n",
    "                dataset_dict[rule]['negative_example'].add(body)\n",
    "        \n",
    "        # Add explicit examples\n",
    "        for i in range(1, 3):\n",
    "            pos_col = f'positive_example_{i}'\n",
    "            if pos_col in train_df.columns:\n",
    "                text = str(row[pos_col]).strip() if pd.notna(row[pos_col]) else \"\"\n",
    "                if text:\n",
    "                    dataset_dict[rule]['positive_example'].add(text)\n",
    "            \n",
    "            neg_col = f'negative_example_{i}'\n",
    "            if neg_col in train_df.columns:\n",
    "                text = str(row[neg_col]).strip() if pd.notna(row[neg_col]) else \"\"\n",
    "                if text:\n",
    "                    dataset_dict[rule]['negative_example'].add(text)\n",
    "    \n",
    "    # Process test.csv - only explicit examples\n",
    "    print(\"Processing test.csv examples...\")\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        rule = row['rule']\n",
    "        \n",
    "        for i in range(1, 3):\n",
    "            pos_col = f'positive_example_{i}'\n",
    "            if pos_col in test_df.columns:\n",
    "                text = str(row[pos_col]).strip() if pd.notna(row[pos_col]) else \"\"\n",
    "                if text:\n",
    "                    dataset_dict[rule]['positive_example'].add(text)\n",
    "            \n",
    "            neg_col = f'negative_example_{i}'\n",
    "            if neg_col in test_df.columns:\n",
    "                text = str(row[neg_col]).strip() if pd.notna(row[neg_col]) else \"\"\n",
    "                if text:\n",
    "                    dataset_dict[rule]['negative_example'].add(text)\n",
    "    \n",
    "    # Convert sets to lists\n",
    "    result_dict = {}\n",
    "    for rule, value in dataset_dict.items():\n",
    "        result_dict[rule] = {\n",
    "            'positive_example': list(value['positive_example']),\n",
    "            'negative_example': list(value['negative_example'])\n",
    "        }\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def build_inference_data(data_path_test: str):\n",
    "    \"\"\"\n",
    "    Build inference data from test.csv body column (nonexamples)\n",
    "    \n",
    "    Returns list of dicts with:\n",
    "    [\n",
    "        {\n",
    "            'row_id': row_id,\n",
    "            'rule': rule text,\n",
    "            'text': body text\n",
    "        }\n",
    "    ]\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(data_path_test)\n",
    "    \n",
    "    inference_data = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        body = str(row['body']).strip() if pd.notna(row['body']) else \"\"\n",
    "        if body:\n",
    "            inference_data.append({\n",
    "                'row_id': row['row_id'],\n",
    "                'rule': row['rule'],\n",
    "                'text': body\n",
    "            })\n",
    "    \n",
    "    return inference_data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Training Dataset\n",
    "# ============================================================================\n",
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Training dataset that creates prompts with examples and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset_dict,\n",
    "        tokenizer,\n",
    "        num_pos_examples: int = 2,\n",
    "        num_neg_examples: int = 2,\n",
    "        sys_prompt: str = \"You are a content moderator for Reddit. Your task is to determine if comments violate community rules. Use the examples as guidance. Answer only 'Yes' if it violates the rule, or 'No' if it doesn't.\",\n",
    "        seed: int = None\n",
    "    ):\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_pos_examples = num_pos_examples\n",
    "        self.num_neg_examples = num_neg_examples\n",
    "        self.sys_prompt = sys_prompt\n",
    "        \n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        \n",
    "        # Create training examples\n",
    "        self.examples = []\n",
    "        \n",
    "        for rule, categories in dataset_dict.items():\n",
    "            pos_examples = categories['positive_example']\n",
    "            neg_examples = categories['negative_example']\n",
    "            \n",
    "            # Add positive examples as training targets\n",
    "            for text in pos_examples:\n",
    "                self.examples.append({\n",
    "                    'rule': rule,\n",
    "                    'text': text,\n",
    "                    'label': 1,\n",
    "                    'pos_pool': [p for p in pos_examples if p != text],\n",
    "                    'neg_pool': neg_examples\n",
    "                })\n",
    "            \n",
    "            # Add negative examples as training targets\n",
    "            for text in neg_examples:\n",
    "                self.examples.append({\n",
    "                    'rule': rule,\n",
    "                    'text': text,\n",
    "                    'label': 0,\n",
    "                    'pos_pool': pos_examples,\n",
    "                    'neg_pool': [n for n in neg_examples if n != text]\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def _build_prompt_text(self, example, sampled_pos_examples, sampled_neg_examples):\n",
    "        \"\"\"Build prompt with rule, examples, and text to classify\"\"\"\n",
    "        prompt_parts = [\n",
    "            self.sys_prompt,\n",
    "            \"\",\n",
    "            f\"Rule: \\\"{example['rule']}\\\"\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        if sampled_pos_examples:\n",
    "            prompt_parts.append(\"Here are examples of comments that VIOLATE this rule:\")\n",
    "            for i, pos_ex in enumerate(sampled_pos_examples, 1):\n",
    "                prompt_parts.append(f\"{i}. {pos_ex}\")\n",
    "            prompt_parts.append(\"\")\n",
    "        \n",
    "        if sampled_neg_examples:\n",
    "            prompt_parts.append(\"Here are examples of comments that DO NOT violate this rule:\")\n",
    "            for i, neg_ex in enumerate(sampled_neg_examples, 1):\n",
    "                prompt_parts.append(f\"{i}. {neg_ex}\")\n",
    "            prompt_parts.append(\"\")\n",
    "        \n",
    "        prompt_parts.extend([\n",
    "            \"Now, evaluate this comment:\",\n",
    "            f\"\\\"{example['text']}\\\"\",\n",
    "            \"\",\n",
    "            \"Does it violate the rule?\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(prompt_parts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Sample examples for prompt context\n",
    "        sampled_pos = random.sample(example['pos_pool'], \n",
    "                                   min(self.num_pos_examples, len(example['pos_pool']))) if example['pos_pool'] else []\n",
    "        sampled_neg = random.sample(example['neg_pool'], \n",
    "                                   min(self.num_neg_examples, len(example['neg_pool']))) if example['neg_pool'] else []\n",
    "        \n",
    "        user_content = self._build_prompt_text(example, sampled_pos, sampled_neg)\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        \n",
    "        text_with_template = self.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "        \n",
    "        # Add label\n",
    "        label = \"Yes\" if example['label'] == 1 else \"No\"\n",
    "        full_text = text_with_template + label + self.tokenizer.eos_token\n",
    "        \n",
    "        # Tokenize the full text\n",
    "        tokenized = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "            return_tensors=None,  # Return lists instead of tensors\n",
    "        )\n",
    "        \n",
    "        return tokenized\n",
    "\n",
    "\n",
    "class ClassifyDataset(Dataset):\n",
    "    \"\"\"Wrapper dataset for classification\"\"\"\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.base_dataset[idx]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Inference Functions\n",
    "# ============================================================================\n",
    "def get_yes_no_probabilities(model, tokenizer, prompts):\n",
    "    \"\"\"\n",
    "    Get probabilities for Yes/No tokens\n",
    "    \"\"\"\n",
    "    yes_token_id = tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "    no_token_id = tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "    \n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    \n",
    "    yes_logits = last_token_logits[:, yes_token_id]\n",
    "    no_logits = last_token_logits[:, no_token_id]\n",
    "    \n",
    "    yes_no_logits = torch.stack([no_logits, yes_logits], dim=1)\n",
    "    probabilities = F.softmax(yes_no_logits, dim=1)\n",
    "    \n",
    "    yes_probs = probabilities[:, 1].cpu().numpy()\n",
    "    return yes_probs\n",
    "\n",
    "\n",
    "def build_inference_prompt(rule, text, pos_examples, neg_examples, sys_prompt, tokenizer, num_pos=2, num_neg=2):\n",
    "    \"\"\"Build prompt for inference\"\"\"\n",
    "    # Sample examples\n",
    "    sampled_pos = random.sample(pos_examples, min(num_pos, len(pos_examples))) if pos_examples else []\n",
    "    sampled_neg = random.sample(neg_examples, min(num_neg, len(neg_examples))) if neg_examples else []\n",
    "    \n",
    "    prompt_parts = [\n",
    "        sys_prompt,\n",
    "        \"\",\n",
    "        f\"Rule: \\\"{rule}\\\"\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    if sampled_pos:\n",
    "        prompt_parts.append(\"Here are examples of comments that VIOLATE this rule:\")\n",
    "        for i, pos_ex in enumerate(sampled_pos, 1):\n",
    "            prompt_parts.append(f\"{i}. {pos_ex}\")\n",
    "        prompt_parts.append(\"\")\n",
    "    \n",
    "    if sampled_neg:\n",
    "        prompt_parts.append(\"Here are examples of comments that DO NOT violate this rule:\")\n",
    "        for i, neg_ex in enumerate(sampled_neg, 1):\n",
    "            prompt_parts.append(f\"{i}. {neg_ex}\")\n",
    "        prompt_parts.append(\"\")\n",
    "    \n",
    "    prompt_parts.extend([\n",
    "        \"Now, evaluate this comment:\",\n",
    "        f\"\\\"{text}\\\"\",\n",
    "        \"\",\n",
    "        \"Does it violate the rule?\"\n",
    "    ])\n",
    "    \n",
    "    user_content = \"\\n\".join(prompt_parts)\n",
    "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Script\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Qwen-3 4B AWQ Training and Inference Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 1: Build dataset\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n[Step 1/5] Building training dataset...\")\n",
    "dataset_dict = build_training_dataset_dict(DATA_PATH_TRAIN, DATA_PATH_TEST, seed=SEED)\n",
    "print(f\"Total rules: {len(dataset_dict)}\")\n",
    "\n",
    "total_pos = sum(len(v['positive_example']) for v in dataset_dict.values())\n",
    "total_neg = sum(len(v['negative_example']) for v in dataset_dict.values())\n",
    "print(f\"Total positive examples: {total_pos}\")\n",
    "print(f\"Total negative examples: {total_neg}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 2: Initialize tokenizer\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n[Step 2/5] Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 3: Create training dataset\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n[Step 3/5] Creating training dataset...\")\n",
    "train_base_dataset = TrainingDataset(\n",
    "    dataset_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    num_pos_examples=NUM_POS_EXAMPLES,\n",
    "    num_neg_examples=NUM_NEG_EXAMPLES,\n",
    "    seed=SEED\n",
    ")\n",
    "train_dataset = ClassifyDataset(train_base_dataset)\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\"Does it violate the rule?\", tokenizer=tokenizer)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 4: Train model\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n[Step 4/5] Training model...\")\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    bias='none',\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_PATH,\n",
    "    logging_steps=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=EPOCH,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    bf16=is_torch_bf16_gpu_available(),\n",
    "    fp16=not is_torch_bf16_gpu_available(),\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC_NUM,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    group_by_length=False,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer_output = trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final training loss: {trainer_output.training_loss:.4f}\")\n",
    "print(f\"Saving model to: {MODEL_OUTPUT_PATH}\")\n",
    "trainer.save_model(MODEL_OUTPUT_PATH)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Step 5: Inference\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n[Step 5/5] Running inference on test.csv...\")\n",
    "\n",
    "# Load inference data\n",
    "inference_data = build_inference_data(DATA_PATH_TEST)\n",
    "print(f\"Total inference samples: {len(inference_data)}\")\n",
    "\n",
    "# Load trained model\n",
    "print(\"Loading trained model...\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ").to(device)\n",
    "\n",
    "trained_model = PeftModel.from_pretrained(base_model, MODEL_OUTPUT_PATH)\n",
    "trained_model.eval()\n",
    "\n",
    "# Prepare prompts\n",
    "print(\"Preparing prompts...\")\n",
    "sys_prompt = \"You are a content moderator for Reddit. Your task is to determine if comments violate community rules. Use the examples as guidance. Answer only 'Yes' if it violates the rule, or 'No' if it doesn't.\"\n",
    "\n",
    "all_prompts = []\n",
    "all_row_ids = []\n",
    "\n",
    "for item in tqdm(inference_data):\n",
    "    rule = item['rule']\n",
    "    text = item['text']\n",
    "    row_id = item['row_id']\n",
    "    \n",
    "    # Get examples for this rule\n",
    "    pos_examples = dataset_dict.get(rule, {}).get('positive_example', [])\n",
    "    neg_examples = dataset_dict.get(rule, {}).get('negative_example', [])\n",
    "    \n",
    "    prompt = build_inference_prompt(\n",
    "        rule, text, pos_examples, neg_examples, \n",
    "        sys_prompt, tokenizer, NUM_POS_EXAMPLES, NUM_NEG_EXAMPLES\n",
    "    )\n",
    "    \n",
    "    all_prompts.append(prompt)\n",
    "    all_row_ids.append(row_id)\n",
    "\n",
    "# Run inference in batches\n",
    "print(\"Running inference...\")\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(all_prompts), BATCH_SIZE)):\n",
    "        batch_prompts = all_prompts[i:i + BATCH_SIZE]\n",
    "        batch_probs = get_yes_no_probabilities(trained_model, tokenizer, batch_prompts)\n",
    "        all_probabilities.extend(batch_probs)\n",
    "\n",
    "# Create submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'row_id': all_row_ids,\n",
    "    'rule_violation': all_probabilities\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission3.csv', index=False)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Pipeline complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Submission saved to: submission3.csv\")\n",
    "print(f\"Total predictions: {len(submission_df)}\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(submission_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a4331e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T09:39:49.285135Z",
     "iopub.status.busy": "2025-10-23T09:39:49.284913Z",
     "iopub.status.idle": "2025-10-23T09:39:54.364811Z",
     "shell.execute_reply": "2025-10-23T09:39:54.363742Z"
    },
    "papermill": {
     "duration": 5.084763,
     "end_time": "2025-10-23T09:39:54.366226",
     "exception": false,
     "start_time": "2025-10-23T09:39:49.281463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset has only 10 rows (less than 20)\n",
      "Generating dummy submission.csv with all 0.5 probabilities...\n",
      "Dummy submission.csv created successfully!\n",
      "   row_id  rule_violation\n",
      "0    2029             0.5\n",
      "1    2030             0.5\n",
      "2    2031             0.5\n",
      "3    2032             0.5\n",
      "4    2033             0.5\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "# Check if test.csv has less than 20 rows\n",
    "test_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')  # Adjust path if needed\n",
    "\n",
    "if len(test_df) < 20:\n",
    "    print(f\"Test dataset has only {len(test_df)} rows (less than 20)\")\n",
    "    print(\"Generating dummy submission.csv with all 0.5 probabilities...\")\n",
    "    \n",
    "    # Create dummy submission with 0.5 for all rows\n",
    "    dummy_submission = pd.DataFrame({\n",
    "        'row_id': test_df['row_id'],\n",
    "        'rule_violation': [0.5] * len(test_df)\n",
    "    })\n",
    "    \n",
    "    dummy_submission.to_csv('submission.csv', index=False)\n",
    "    print(\"Dummy submission.csv created successfully!\")\n",
    "    print(dummy_submission.head())\n",
    "    \n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    p1 = subprocess.Popen(['python', 'train_on_gpu_0_gemma3.py'])\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    p2 = subprocess.Popen(['python', 'train_on_gpu_1_qwen3.py'])\n",
    "    \n",
    "    p1.wait()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Gemma3 training completed on GPU 0!\")\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    p3 = subprocess.Popen(['python', 'train_on_gpu_0_bert.py'])\n",
    "    \n",
    "    p3.wait()\n",
    "    print(\"BERT training completed on GPU 0!\")\n",
    "    \n",
    "    # Wait for Qwen3 to finally complete\n",
    "    p2.wait()\n",
    "    print(\"Qwen3 training completed on GPU 1!\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"\\nAll scripts completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3046a58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T09:39:54.373769Z",
     "iopub.status.busy": "2025-10-23T09:39:54.372985Z",
     "iopub.status.idle": "2025-10-23T09:39:54.407261Z",
     "shell.execute_reply": "2025-10-23T09:39:54.406347Z"
    },
    "papermill": {
     "duration": 0.03896,
     "end_time": "2025-10-23T09:39:54.408346",
     "exception": true,
     "start_time": "2025-10-23T09:39:54.369386",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'submission1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_175/2973775471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load submission files (fixed typo: 'submission')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msubmission_df_gemma3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msubmission_df_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msubmission_df_qwen3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'submission1.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load submission files (fixed typo: 'submission')\n",
    "submission_df_gemma3 = pd.read_csv('submission1.csv')\n",
    "submission_df_bert = pd.read_csv('submission2.csv')\n",
    "submission_df_qwen3 = pd.read_csv('submission3.csv')\n",
    "\n",
    "# Calculate fractional ranks\n",
    "r_gemma3 = submission_df_gemma3['rule_violation'].rank(method='average') / (len(submission_df_gemma3) + 1)\n",
    "r_bert = submission_df_bert['rule_violation'].rank(method='average') / (len(submission_df_bert) + 1)\n",
    "r_qwen3 = submission_df_qwen3['rule_violation'].rank(method='average') / (len(submission_df_qwen3) + 1)\n",
    "\n",
    "# Combine the ranks (fixed logic: used r_gemma3 instead of r_qwen3 twice)\n",
    "blend = 0.4 * r_qwen3 + 0.3 * r_bert + 0.3 * r_gemma3\n",
    "\n",
    "# Create the final submission DataFrame (fixed: defined 'q' before use)\n",
    "# This copies the structure (e.g., ID columns) from one of the submissions\n",
    "q = submission_df_gemma3.copy()\n",
    "q['rule_violation'] = blend\n",
    "\n",
    "# Save the final submission\n",
    "q.to_csv('submission.csv', index=False)\n",
    "print(q.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be95f2e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 2663421,
     "sourceId": 4620664,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8540792,
     "sourceId": 13455162,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8540854,
     "sourceId": 13457915,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301515,
     "sourceId": 363135,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 322458,
     "sourceId": 391622,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 322459,
     "sourceId": 391623,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.762168,
   "end_time": "2025-10-23T09:39:55.128890",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-23T09:39:45.366722",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
