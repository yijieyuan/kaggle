{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e6b9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: Identifying series in segmentations and series directories\n",
      "================================================================================\n",
      "\n",
      "Series in segmentations: 178\n",
      "Series in series: 4348\n",
      "Series without segmentations: 4171\n",
      "Already processed: 0\n",
      "\n",
      "================================================================================\n",
      "PROCESSING PLAN:\n",
      "================================================================================\n",
      "Phase 2 - Series with segmentations to process: 177\n",
      "Phase 3 - Series without segmentations to process: 4150\n",
      "Total to process: 4327\n",
      "Skipping (already processed): 0\n",
      "Skipping (invalid): 22\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: Processing series WITH segmentations\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PHASE 2: Processing series WITH segmentations:  45%|████▍     | 79/177 [08:14<07:22,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Series folder not found: E:\\data_old\\series\\1.2.826.0.1.3680043.8.498.43502795339700498960289295234851562632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PHASE 2: Processing series WITH segmentations:  66%|██████▌   | 116/177 [10:05<02:59,  2.94s/it]c:\\Users\\yyuan57\\Anaconda3\\envs\\yyuan57\\lib\\site-packages\\pydicom\\pixel_data_handlers\\util.py:1146: UserWarning: A value of None for (0028,0008) 'Number of Frames' is non-conformant. It's recommended that this value be changed to 1\n",
      "  warnings.warn(\"A value of None for (0028,0008) 'Number of Frames' is \"\n",
      "PHASE 2: Processing series WITH segmentations: 100%|██████████| 177/177 [14:32<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3: Processing series WITHOUT segmentations\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PHASE 3: Processing series WITHOUT segmentations: 100%|██████████| 4150/4150 [7:46:20<00:00,  6.74s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "Series with segmentations processed: 176\n",
      "Series without segmentations processed: 4150\n",
      "Total successfully processed this run: 4326\n",
      "Total already processed (skipped): 0\n",
      "Total invalid (skipped): 22\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "def should_rescale_ct(ds, pixel_array):\n",
    "    \"\"\"Determine if CT should be rescaled\"\"\"\n",
    "    if ds.get('Modality', '') != 'CT':\n",
    "        return False\n",
    "    if not (hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept')):\n",
    "        return False\n",
    "    min_pixel = pixel_array.min()\n",
    "    if min_pixel >= -100 or min_pixel == -2000:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_direction_label(vec):\n",
    "    \"\"\"Convert orientation vector to direction label\"\"\"\n",
    "    abs_vec = np.abs(vec)\n",
    "    dominant_idx = np.argmax(abs_vec)\n",
    "    dominant_val = vec[dominant_idx]\n",
    "    \n",
    "    if dominant_idx == 0:  # X axis\n",
    "        return 'RL' if dominant_val > 0 else 'LR'\n",
    "    elif dominant_idx == 1:  # Y axis\n",
    "        return 'AP' if dominant_val > 0 else 'PA'\n",
    "    else:  # Z axis\n",
    "        return 'FH' if dominant_val > 0 else 'HF'\n",
    "\n",
    "def determine_orientation(iop):\n",
    "    \"\"\"Determine row and column directions from ImageOrientationPatient\"\"\"\n",
    "    row_vec = np.array(iop[:3])\n",
    "    col_vec = np.array(iop[3:6])\n",
    "    \n",
    "    dim2_dir = get_direction_label(row_vec)\n",
    "    dim1_dir = get_direction_label(col_vec)\n",
    "    \n",
    "    dim2_axis = 0 if dim2_dir in ['LR', 'RL'] else (1 if dim2_dir in ['AP', 'PA'] else 2)\n",
    "    dim1_axis = 0 if dim1_dir in ['LR', 'RL'] else (1 if dim1_dir in ['AP', 'PA'] else 2)\n",
    "    \n",
    "    used_axes = {dim2_axis, dim1_axis}\n",
    "    dim0_axis = (set([0, 1, 2]) - used_axes).pop()\n",
    "    dim0_dir = ['RL', 'AP', 'FH'][dim0_axis]\n",
    "    \n",
    "    return dim2_dir, dim1_dir, dim0_dir\n",
    "\n",
    "def resize_volume_xy_only(volume, target_xy=256):\n",
    "    \"\"\"Resize volume to 256x256 in x,y dimensions, keep z unchanged\"\"\"\n",
    "    # Calculate zoom factors: keep z (dim 0) unchanged, resize x,y (dims 1,2) to 256\n",
    "    zoom_factors = [1.0, target_xy / volume.shape[1], target_xy / volume.shape[2]]\n",
    "    resized = zoom(volume, zoom_factors, order=1)\n",
    "    return resized\n",
    "\n",
    "def parse_coordinates(coord_str):\n",
    "    \"\"\"Parse coordinate string to dictionary\"\"\"\n",
    "    return ast.literal_eval(coord_str)\n",
    "\n",
    "def get_slice_position(ds):\n",
    "    \"\"\"Get slice position for sorting\"\"\"\n",
    "    return float(ds.ImagePositionPatient[2])\n",
    "\n",
    "def build_sop_to_slice_mapping(series_folder):\n",
    "    \"\"\"Build mapping of SOP UID to slice index by sorting DICOM files\"\"\"\n",
    "    dcm_files = list(Path(series_folder).glob(\"*.dcm\"))\n",
    "    \n",
    "    is_multiframe_series = len(dcm_files) == 1\n",
    "    \n",
    "    rows, cols = None, None\n",
    "    num_slices = 0\n",
    "    sop_to_slice_mapping = {}\n",
    "    \n",
    "    if is_multiframe_series:\n",
    "        dcm_file = dcm_files[0]\n",
    "        ds = pydicom.dcmread(dcm_file)\n",
    "        pixel_array = ds.pixel_array\n",
    "        \n",
    "        rows = ds.Rows\n",
    "        cols = ds.Columns\n",
    "        \n",
    "        assert len(pixel_array.shape) == 3\n",
    "        num_slices = pixel_array.shape[0]\n",
    "        \n",
    "        if 'NumberOfFrames' in ds:\n",
    "            assert num_slices == int(ds.NumberOfFrames)\n",
    "    else:\n",
    "        slice_data = []\n",
    "        \n",
    "        for dcm_file in dcm_files:\n",
    "            ds = pydicom.dcmread(dcm_file)\n",
    "            \n",
    "            if rows is None:\n",
    "                rows = ds.Rows\n",
    "                cols = ds.Columns\n",
    "            \n",
    "            sop_uid = ds.SOPInstanceUID\n",
    "            position = get_slice_position(ds)\n",
    "            \n",
    "            slice_data.append({\n",
    "                'sop_uid': sop_uid,\n",
    "                'position': position\n",
    "            })\n",
    "        \n",
    "        slice_data.sort(key=lambda x: x['position'])\n",
    "        \n",
    "        for idx, slice_info in enumerate(slice_data):\n",
    "            sop_to_slice_mapping[slice_info['sop_uid']] = idx\n",
    "        \n",
    "        num_slices = len(slice_data)\n",
    "    \n",
    "    return sop_to_slice_mapping, num_slices, rows, cols, is_multiframe_series\n",
    "\n",
    "def is_series_valid(series_uid):\n",
    "    \"\"\"Check if a series is valid for processing\"\"\"\n",
    "    if series_uid in INVALID_SERIES_SET:\n",
    "        return False, \"invalid_series\"\n",
    "    return True, series_uid\n",
    "\n",
    "def process_series_with_crop(series_path, localizers_df):\n",
    "    \"\"\"Process a single series folder with cropping and label computation\"\"\"\n",
    "    dcm_files = list(Path(series_path).glob('*.dcm'))\n",
    "    \n",
    "    # Skip if only one file (multi-frame)\n",
    "    # if len(dcm_files) <= 1:\n",
    "        # return None\n",
    "    \n",
    "    # Read first DICOM to get orientation\n",
    "    ds = pydicom.dcmread(dcm_files[0])\n",
    "    series_uid = ds.SeriesInstanceUID\n",
    "    modality = ds.get('Modality', 'Unknown')\n",
    "    \n",
    "    # iop = ds.ImageOrientationPatient\n",
    "    # dim2_dir, dim1_dir, dim0_dir = determine_orientation(iop)\n",
    "    \n",
    "    # Build SOP mapping for label calculation\n",
    "    sop_to_slice_mapping, num_slices, rows, cols, is_multiframe = build_sop_to_slice_mapping(series_path)\n",
    "    \n",
    "    # Read all slices and sort by Z position\n",
    "    if not is_multiframe:\n",
    "        slices = []\n",
    "        for dcm_file in dcm_files:\n",
    "            ds = pydicom.dcmread(dcm_file)\n",
    "            z_pos = ds.ImagePositionPatient[2]\n",
    "            slices.append((z_pos, ds))\n",
    "        \n",
    "        slices.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Stack volume\n",
    "        volume = []\n",
    "        for z_pos, ds in slices:\n",
    "            pixel_array = ds.pixel_array.astype(np.float32)\n",
    "            \n",
    "            # Apply rescale for CT if needed\n",
    "            if should_rescale_ct(ds, pixel_array):\n",
    "                pixel_array = pixel_array * ds.RescaleSlope + ds.RescaleIntercept\n",
    "            \n",
    "            volume.append(pixel_array)\n",
    "        \n",
    "        volume = np.stack(volume, axis=0)\n",
    "    else:\n",
    "        ds = pydicom.dcmread(dcm_files[0])\n",
    "        pixel_array = ds.pixel_array.astype(np.float32)\n",
    "        \n",
    "        # Apply rescale for CT if needed\n",
    "        if should_rescale_ct(ds, pixel_array):\n",
    "            pixel_array = pixel_array * ds.RescaleSlope + ds.RescaleIntercept\n",
    "        \n",
    "        volume = pixel_array  # shape (num_slices, rows, cols)\n",
    "\n",
    "    # Record original size\n",
    "    original_shape = volume.shape\n",
    "\n",
    "    # Crop at dim0 [-600:, :, :]\n",
    "    crop_start = max(0, volume.shape[0] - 600)\n",
    "    volume_cropped = volume[crop_start:, :, :]\n",
    "    cropped_shape = volume_cropped.shape\n",
    "    \n",
    "    # Calculate crop offset for coordinate adjustment\n",
    "    crop_offset = crop_start\n",
    "    \n",
    "    # Clip CT\n",
    "    if modality == 'CT':\n",
    "        volume_cropped = np.clip(volume_cropped, -200, 600)\n",
    "    \n",
    "    # Normalization\n",
    "    if modality == 'CT':\n",
    "        vmin, vmax = volume_cropped.min(), volume_cropped.max()\n",
    "    else:\n",
    "        # Min-max normalization for MR\n",
    "        vmin, vmax = np.percentile(volume_cropped, 0.5), np.percentile(volume_cropped, 99.5)\n",
    "        volume_cropped = np.clip(volume_cropped, vmin, vmax)\n",
    "    \n",
    "    if vmax > vmin:\n",
    "        volume_cropped = (volume_cropped - vmin) / (vmax - vmin)\n",
    "    else:\n",
    "        volume_cropped = np.zeros_like(volume_cropped)\n",
    "    \n",
    "    # Calculate percentage coordinates after cropping\n",
    "    series_localizers = localizers_df[localizers_df['SeriesInstanceUID'] == series_uid]\n",
    "    percentage_coords = []\n",
    "    \n",
    "    if len(series_localizers) > 0:\n",
    "        for _, loc_row in series_localizers.iterrows():\n",
    "            sop_uid = loc_row['SOPInstanceUID']\n",
    "            coords = parse_coordinates(loc_row['coordinates'])\n",
    "            \n",
    "            if coords is None:\n",
    "                continue\n",
    "            \n",
    "            x = coords['x']\n",
    "            y = coords['y']\n",
    "            \n",
    "            # Determine z index\n",
    "            if is_multiframe:\n",
    "                z_idx = int(coords['f']) - 1\n",
    "            else:\n",
    "                z_idx = sop_to_slice_mapping.get(sop_uid)\n",
    "                if z_idx is None:\n",
    "                    continue\n",
    "            \n",
    "            # Adjust z index after cropping\n",
    "            z_idx_cropped = z_idx - crop_offset\n",
    "            \n",
    "            # Skip if the coordinate is outside the cropped region\n",
    "            if z_idx_cropped < 0 or z_idx_cropped >= cropped_shape[0]:\n",
    "                continue\n",
    "            \n",
    "            # Calculate percentage coordinates based on cropped volume\n",
    "            x_pct = x / cols\n",
    "            y_pct = y / rows\n",
    "            z_pct = z_idx_cropped / cropped_shape[0]\n",
    "            \n",
    "            percentage_coords.append([x_pct, y_pct, z_pct])\n",
    "    \n",
    "    # Resize to 256x256 in x,y only, keep z unchanged\n",
    "    volume_resized = resize_volume_xy_only(volume_cropped, target_xy=256)\n",
    "    \n",
    "    # Scale to 0-255 and convert to uint8\n",
    "    volume_uint8 = (volume_cropped * 255).astype(np.uint8)\n",
    "    volume_uint8_256_z = (volume_resized * 255).astype(np.uint8)\n",
    "    \n",
    "    return {\n",
    "        'SeriesInstanceUID': series_uid,\n",
    "        'Modality': modality,\n",
    "        'original_shape': original_shape,\n",
    "        'cropped_shape': cropped_shape,\n",
    "        'volume_uint8': volume_uint8,\n",
    "        'volume_uint8_256_z': volume_uint8_256_z,\n",
    "        'percentage_coords': percentage_coords\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "segmentations_dir = Path(r'E:\\data_old\\segmentations')\n",
    "series_dir = Path(r'E:\\data_old\\series')\n",
    "output_dir = Path(r'./volume_uint8')\n",
    "output_dir_256_z = Path(r'E:\\kaggle-rsna-data_processing3\\volume_uint8_256_z')\n",
    "label_percentage_folder = Path(r'./label_percentage_z')\n",
    "\n",
    "# Create output directories\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_dir_256_z.mkdir(exist_ok=True, parents=True)\n",
    "label_percentage_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Load localizers CSV\n",
    "localizers_csv_path = r\"E:\\data_old_updated\\train_localizers.csv\"\n",
    "localizers_df = pd.read_csv(localizers_csv_path)\n",
    "\n",
    "# Define invalid series to exclude\n",
    "INVALID_SERIES = [\n",
    "    \"1.2.826.0.1.3680043.8.498.35204126697881966597435252550544407444\",\n",
    "    \"1.2.826.0.1.3680043.8.498.11145695452143851764832708867797988068\",\n",
    "    \"1.2.826.0.1.3680043.8.498.12937082136541515013380696257898978214\",\n",
    "    \"1.2.826.0.1.3680043.8.498.86840850085811129970747331978337342341\",\n",
    "    \"1.2.826.0.1.3680043.8.498.10733938921373716882398209756836684843\",\n",
    "    \"1.2.826.0.1.3680043.8.498.11292203154407642658894712229998766945\",\n",
    "    \"1.2.826.0.1.3680043.8.498.74390569791112039529514861261033590424\",\n",
    "    \"1.2.826.0.1.3680043.8.498.99892390884723813599532075083872271516\",\n",
    "    \"1.2.826.0.1.3680043.8.498.99421822954919332641371697175982753182\",\n",
    "    \"1.2.826.0.1.3680043.8.498.93005379507993862369794871518209403819\",\n",
    "    \"1.2.826.0.1.3680043.8.498.87133443408651185245864983172506753347\",\n",
    "    \"1.2.826.0.1.3680043.8.498.85042275841446604538710616923989532822\",\n",
    "    \"1.2.826.0.1.3680043.8.498.81867770017494605078034950552739870155\",\n",
    "    \"1.2.826.0.1.3680043.8.498.75294325392457179365040684378207706807\",\n",
    "    \"1.2.826.0.1.3680043.8.498.73348230187682293339845869829853553626\",\n",
    "    \"1.2.826.0.1.3680043.8.498.34908224715351895924870591631151425521\",\n",
    "    \"1.2.826.0.1.3680043.8.498.13356606276376861530476731358572238037\",\n",
    "    \"1.2.826.0.1.3680043.8.498.13299935636593758131187104226860563078\",\n",
    "    \"1.2.826.0.1.3680043.8.498.12780687841924878965940656634052376723\",\n",
    "    \"1.2.826.0.1.3680043.8.498.12285352638636973719542944532929535087\",\n",
    "    \"1.2.826.0.1.3680043.8.498.11019101980573889157112037207769236902\",\n",
    "    \"1.2.826.0.1.3680043.8.498.10820472882684587647235099308830427864\",\n",
    "]\n",
    "\n",
    "INVALID_SERIES_SET = set(INVALID_SERIES)\n",
    "\n",
    "# Initialize size CSV\n",
    "size_csv_path = './size.csv'\n",
    "if not Path(size_csv_path).exists():\n",
    "    pd.DataFrame(columns=['SeriesInstanceUID', 'original_dim0', 'original_dim1', 'original_dim2',\n",
    "                          'cropped_dim0', 'cropped_dim1', 'cropped_dim2']).to_csv(size_csv_path, index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: Get series lists from both directories\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: Identifying series in segmentations and series directories\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get series with segmentations\n",
    "segmentation_series = set()\n",
    "if segmentations_dir.exists():\n",
    "    # Find all .nii files that don't end with _cowseg.nii\n",
    "    for f in segmentations_dir.iterdir():\n",
    "        if f.is_file() and f.name.endswith('.nii') and not f.name.endswith('_cowseg.nii'):\n",
    "            # Extract seriesUID by removing the .nii extension\n",
    "            series_uid = f.name.replace('.nii', '')\n",
    "            segmentation_series.add(series_uid)\n",
    "    \n",
    "    print(f\"\\nSeries in segmentations: {len(segmentation_series)}\")\n",
    "else:\n",
    "    print(f\"\\nWarning: Segmentations directory not found: {segmentations_dir}\")\n",
    "\n",
    "# Get all series\n",
    "all_series = set()\n",
    "if series_dir.exists():\n",
    "    all_series = {f.name for f in series_dir.iterdir() if f.is_dir()}\n",
    "    print(f\"Series in series: {len(all_series)}\")\n",
    "else:\n",
    "    print(f\"\\nError: Series directory not found: {series_dir}\")\n",
    "    exit(1)\n",
    "\n",
    "# Series without segmentations\n",
    "series_without_seg = all_series - segmentation_series\n",
    "print(f\"Series without segmentations: {len(series_without_seg)}\")\n",
    "\n",
    "# Check already processed\n",
    "already_processed = set()\n",
    "if output_dir_256_z.exists():\n",
    "    already_processed = {f.stem for f in output_dir_256_z.glob('*.npy')}\n",
    "print(f\"Already processed: {len(already_processed)}\")\n",
    "\n",
    "# Filter out invalid and already processed\n",
    "segmentation_series_to_process = segmentation_series - INVALID_SERIES_SET - already_processed\n",
    "series_without_seg_to_process = series_without_seg - INVALID_SERIES_SET - already_processed\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING PLAN:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Phase 2 - Series with segmentations to process: {len(segmentation_series_to_process)}\")\n",
    "print(f\"Phase 3 - Series without segmentations to process: {len(series_without_seg_to_process)}\")\n",
    "print(f\"Total to process: {len(segmentation_series_to_process) + len(series_without_seg_to_process)}\")\n",
    "print(f\"Skipping (already processed): {len(already_processed & all_series)}\")\n",
    "print(f\"Skipping (invalid): {len(INVALID_SERIES_SET & all_series)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: Process series with segmentations first\n",
    "# ============================================================================\n",
    "\n",
    "def process_batch(series_list, source_dir, batch_name):\n",
    "    \"\"\"Process a batch of series from a given source directory\"\"\"\n",
    "    if len(series_list) == 0:\n",
    "        print(f\"\\nNo series to process in {batch_name}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{batch_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for series_name in tqdm(list(series_list), desc=batch_name):\n",
    "        series_folder = source_dir / series_name\n",
    "        \n",
    "        if not series_folder.exists():\n",
    "            print(f\"\\nWarning: Series folder not found: {series_folder}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            result = process_series_with_crop(series_folder, localizers_df)\n",
    "            if result is not None:\n",
    "                series_uid = result['SeriesInstanceUID']\n",
    "                \n",
    "                # Save volumes\n",
    "                # np.save(output_dir / f\"{series_uid}.npy\", result['volume_uint8'])\n",
    "                np.save(output_dir_256_z / f\"{series_uid}.npy\", result['volume_uint8_256_z'])\n",
    "                \n",
    "                # Save percentage coordinates\n",
    "                if result['percentage_coords']:\n",
    "                    np.save(label_percentage_folder / f\"{series_uid}.npy\",\n",
    "                           np.array(result['percentage_coords'], dtype=np.float32))\n",
    "                \n",
    "                # Save size information\n",
    "                size_info = {\n",
    "                    'SeriesInstanceUID': series_uid,\n",
    "                    'original_dim0': result['original_shape'][0],\n",
    "                    'original_dim1': result['original_shape'][1],\n",
    "                    'original_dim2': result['original_shape'][2],\n",
    "                    'cropped_dim0': result['cropped_shape'][0],\n",
    "                    'cropped_dim1': result['cropped_shape'][1],\n",
    "                    'cropped_dim2': result['cropped_shape'][2]\n",
    "                }\n",
    "                pd.DataFrame([size_info]).to_csv(size_csv_path, mode='a', header=False, index=False)\n",
    "                \n",
    "                results.append({\n",
    "                    'SeriesInstanceUID': series_uid,\n",
    "                    'Modality': result['Modality']\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {series_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process segmentation series first\n",
    "seg_results = process_batch(\n",
    "    segmentation_series_to_process,\n",
    "    series_dir,\n",
    "    \"PHASE 2: Processing series WITH segmentations\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: Process remaining series without segmentations\n",
    "# ============================================================================\n",
    "\n",
    "other_results = process_batch(\n",
    "    series_without_seg_to_process,\n",
    "    series_dir,\n",
    "    \"PHASE 3: Processing series WITHOUT segmentations\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "all_results = seg_results + other_results\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Series with segmentations processed: {len(seg_results)}\")\n",
    "print(f\"Series without segmentations processed: {len(other_results)}\")\n",
    "print(f\"Total successfully processed this run: {len(all_results)}\")\n",
    "print(f\"Total already processed (skipped): {len(already_processed & all_series)}\")\n",
    "print(f\"Total invalid (skipped): {len(INVALID_SERIES_SET & all_series)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc52b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for label files in both label_percentage and label_percentage_z...\n",
      "Label files in label_percentage: 1842\n",
      "Label files in label_percentage_z: 1842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1842/1842 [00:00<00:00, 2330.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# compare file with same name in ./label_percentage_z and ./label_percentage\n",
    "print(\"\\nChecking for label files in both label_percentage and label_percentage_z...\")\n",
    "label_percentage_dir = Path(r'./label_percentage')\n",
    "label_percentage_z_dir = Path(r'./label_percentage_z')\n",
    "label_percentage_files = {f.stem for f in label_percentage_dir.glob('*.npy')}\n",
    "label_percentage_z_files = {f.stem for f in label_percentage_z_dir.glob('*.npy')}\n",
    "common_label_files = label_percentage_files & label_percentage_z_files\n",
    "print(f\"Label files in label_percentage: {len(label_percentage_files)}\")\n",
    "print(f\"Label files in label_percentage_z: {len(label_percentage_z_files)}\")\n",
    "\n",
    "for fname in tqdm(common_label_files):\n",
    "    f1 = np.load(label_percentage_dir / f\"{fname}.npy\")\n",
    "    f2 = np.load(label_percentage_z_dir / f\"{fname}.npy\")\n",
    "    if not np.array_equal(f1, f2):\n",
    "        print(f\"Difference found in label file: {fname}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc57d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yyuan57",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
